-- startign a new notes file in 2022. just trying to get a shift, to see if I can actually make something happen here.

-- got mongodb ported across to the new G14, system seems to run, had already installed Node and other stuff.

-- have to get the next paper work started... maybe test running sequences again first?

or maybe a better idea would be to being collecting references? Build up a database somehow? Online?


--test out the sequences again: do a run of experiments where noise increases each time?

-- performance noise only, go from 0 to 100 in steps of 10?

-- how does the sequence setup work again?


-- example (after running)

{"iterations":2,"active":0,"variations":[{"iterations":[3,2,1],"type":"rider","rider_no":0,"property":"threshold_power","values":[200,300,400]},{"iterations":[3,2,1],"type":"rider","rider_no":0,"property":"max_power","values":[800,900,1000]}],"experiments":[{"client_id":"86.43.200.150_Galway_IE_None_2021-6-14 10:41:47","iteration":2,"status":"complete"},{"client_id":"86.43.200.150_Galway_IE_None_2021-6-14 10:41:47","iteration":1,"status":"complete"}]}

-- strip it back to the basics

{"iterations":11,"active":1,"variations":[{"iterations":[10,9,8,7,6,5,4,3,2,1],"type":"global","property":"performance_failure_enabled","values":[1,1,1,1,1,1,1,1,1,1]},{"iterations":[10,9,8,7,6,5,4,3,2,1],"type":"rider","rider_no":0,"property":"performance_failure_rate","values":[10,9,8,7,6,5,4,3,2,1]},{"iterations":[10,9,8,7,6,5,4,3,2,1],"type":"rider","rider_no":1,"property":"performance_failure_rate","values":[10,9,8,7,6,5,4,3,2,1]},{"iterations":[10,9,8,7,6,5,4,3,2,1],"type":"rider","rider_no":2,"property":"performance_failure_rate","values":[10,9,8,7,6,5,4,3,2,1]},{"iterations":[10,9,8,7,6,5,4,3,2,1],"type":"rider","rider_no":3,"property":"performance_failure_rate","values":[10,9,8,7,6,5,4,3,2,1]}]}

-- hmmm, doesn't seem to work, adds "experiments":[{"client_id":"","iteration":2,"status":"active"}]} so maybe it runs or tries to run once and crashes? One experiment defo ran.

-- Hmmm, browser_client_id is empty so maybe this causes it to end without saving anything?

-- ok, so this is failing:
  $.getJSON('http://www.geoplugin.net/json.gp', function(data) {
  
  jquery-3.4.1.min.js:2 GET http://www.geoplugin.net/json.gp net::ERR_BLOCKED_BY_CLIENT
  
-- so need a different way to get the IP address? Or can I find another way to 

-- huh, seems to work in Postman, so maybe it is the adblocker?

-- turned those off; it is defo doing more  now but still failing :-(

-- problem seems to be Ghostery

-- ok I think  it ran?

-- need to make a bunch of code changes, e.g. more realistic start, better performace failure, need to run whole bunches of experiments and collate lots and lots of results.##


	

get the paper into their format (tomorrow)

special case of scheduling. maybe in gecco. 

sac
gecco 


suggested reviewers.

-- run a sequence again
{"iterations":2,"active":1,"variations":[{"iterations":[2,1],"type":"global","property":"performance_failure_enabled","values":[1,1]},{"iterations":[2,1],"type":"rider","rider_no":0,"property":"performance_failure_rate","values":[10,1]},{"iterations":[2,1],"type":"rider","rider_no":1,"property":"performance_failure_rate","values":[10,1]},{"iterations":[2,1],"type":"rider","rider_no":2,"property":"performance_failure_rate","values":[10,1]},{"iterations":[2,1],"type":"rider","rider_no":3,"property":"performance_failure_rate","values":[10,1]}]}

format for sciendo IJCSS International Journal of Computer Science in Sport (https://sciendo.com/journal/IJCSS)
> Times New Roman size 12 DONE
> fully justified - seems already ok?
> indented by 3cm from both sides - seems already ok? Nope, seems bigger on mine than on the sample. Check another sample or two and compare font/spacing/etc. 

Ok, used the geometry package and it seeems to work now, just visually basing it on the other sample papers

> Number the pages
consecutively, with no line numbering and no 'headers and footers'
> no footnotes

How do I remove the force calculator website footnote? Add it as a reference?

added it as an @online entry in the bib

-- tried to fix up the one table, improved but probably  not perfect. need to look again at the online citation, think it has no year showing?

-- oh nope it's fine.


> headers:
“Section_Headline_IJCSS” (Arial 12, bold),
“Subsection_Headline_IJCSS” (Arial 12, bold and italic) and
“Subsubsection_Headline_IJCSS” (Times New Roman 12, italic)

-- right, lotsa work there,  has it improved? sore eyes again.

> title
Title and subtitle are separated by a hyphen “-“. Each word starts with a capital letter, except
conjunction such as “and”, “or”, “a”, “on” etc. Use font Times New Roman size 18 and bold
style print (see Example 1).

> authors
Authors and institution must be written in size 12 and in italics. Sur- and forename of multiple
authors are separated by a comma. The first name(s) is/are abbreviated with the first letter of
the name. eg. Novatchkov, H., Bichler, S., Tampier, M., Kornfeind, P.
Department of Biomechanics, Kinesiology and Computer Science,
Faculty of Sport Science,University of Vienna,
Auf der Schmelz 6A, 1150 Vienna, Austria

> abstract
The abstract must not exceed 200 words and it should summarize the paper, giving a clear
indication of the conclusions it contains. It must be inserted in the article after the authors’
addresses, indented by 1 cm from both sides of the normal text. The abstract must not contain
figures or tables (see Example 4)

left-align abstract header

-- remove paragraph indentation
-- done

-- why do the images take up whole pages in some places?

-- right, have to finish tthis paper reformat, need to add in their seciton headers, but where? feel lost right now
Introduction
Methods
Results
Discussion
Conclusion

-- guess I just have to try find the most natural home for them and go form there, email Colm tomorrow and mention that?



From: https://tex.stackexchange.com/questions/69869/image-taking-up-full-page
Most likely it ended up being a float page p because the constraints on size stopped it being a t float. in which case change [htp] which allows p and constrains the number and size of floats in the t area to [!ht] which does not allow p and removes the numeric and size constraints.

If that isn't sufficient, edit the question to add a complete example which shows the problem.

-- adjust other image placements?


-- need to put fulls stops and not commas to get the proper APA format in the references?

-- maybe usepackage{apacite} ?

-- took time but i think? the apa format is now ok... certainly looks like the sample ones. 

-- need to put in Methods, Discussion, and Results and fit everything under those headers? But my stuff doesn't map to those does it?




-- meeting with COlm... 

idea for paper 2
exploration of noise and robustness
-- try out different kinds of failure and noise and find ways to measure their robustness

-- improve the performance failure and the size of the failures (don't tie them directly to the rate of failure)
-- find different ways to work out robustness and compare strategies.

-- e.g. 1: evolve strategies in a  noisy environment, as I have done
-- 2, evolve in a clean environment and then somehow 'add perturbations' to the final strategy, I guess to improve robustness? Or do I use mutations to measure it as I had done before?

-- need a form of noise to model a coach's lack of knowledge about a rider, or a general ambiguity about a rider's actual power, which at the beginning of any race may vary within some range instead of being a fixed point

-- need to be able to add enough noise in different ways to make it totally chaotic, or serene, and all of these have to be replayable as before.


-- how do I take an evolved strategy and consider how risky it is? E.g. evolve one then run it with some rider failure. And some noise. How Bout just small changes in the timing. 




-- monday feb 7th.
--  need to begin the next piece of work and paper... no idea what will happen if paper 1 doesn't get accepted. but what can ya do?

-- need a title... Robustness and Noise in a Genetic Algorithm Search for Simulated Track Cycling Strategies?  Good enough for now.

-- created a new doc and added a title and abstract.

-- first extend the performance failure and add the 'hot headed' noise?

-- yup yup, get coding, but make the new form of performance failure optional so the old stuff for now is still repeatable.

-- added  "performance_failure_effect_type": 2,#-- type 1 can be the old one, and the default, and type 2 can be a new version 

-- so I can add this in calculate_rider_performance_failure_percentage_amount()


-- how do i see again if a setting exists and default it if not?

-- ran a test version but there's not a ton of failure happening- want to max it out to see it... 

-- yeah more failure, tho not really maxxxing it out, I wanna see mad chaos when everyone keeps failing all the time?

-- maybe I can make the performance failure larger, i.e. that they can fail by more... what's the max % they can fail by now?

-- hmmmm, the value seems > 1, that's mad odd

-- yikes, getting very high fatigue values... wonder what those races are up to? 

-- looks like race_rider.endurance_fatigue_level gets very high... but how can it if a rider is supposed to recover after crossing their limit?

effort 4 effort_max 9 current_fatigue 181.42766666666668 current_fatigue_max 100
 accumulated_fatigue 181.42766666666668
 accumulated_fatigue_max 500 rider_performance_failure_multiplier 10 rider_performance_failure_multiplier_max 10
 performance_failure_base_max_percentage 1 performance_failure_amount_exponent 2
 performance_failure_effort_importance_multiplier 1 failure_type 2
 
  accumulated_effect = (settings_r.accumulated_fatigue_maximum - race_rider.accumulated_fatigue)/settings_r.accumulated_fatigue_maximum;
  
  failure_level = settings_r.fatigue_failure_level*accumulated_effect;
  
  accumulated_effect = (500 - 181.42766666666668)/500;
  failure_level = 100*0.6371446666666666;
  = 63.71446666666666
    if(race_rider.endurance_fatigue_level >= failure_level){
if (181.42766666666668 > 63.71446666666666)	
-- how can fatigue get so high like this?
-- maybe it is sometimes adding a ton of fatigue in one go?
 let fatigue_rise = race_rider.fatigue_rate*Math.pow(( (race_rider.power_out- race_rider.threshold_power)/(race_rider.max_power-race_rider.threshold_power)),settings_r.fatigue_power_rate);
 
 -- max fatigue rise doe snot look so high, so it is being let grow very high... how so?
 -- does it happen without noise?
 -- fatigue_rise 15.563999999999998 max wihout performance failure... but in this case I don't see the max failure level? That's what is driving the effect?
 
 
-- ok, so very high fatigu can accumulate even with no noise at all. need to store the settings of a race that has this (instructions and start order only needed as I know which set of settings are being used....)

-- can i replicate this race?
max_endurance_fatigue_level 182.16366666666664 fatigue_rise 0.9056666666666673 race_rider.fatigue_rate 20 race_rider.power_out 427.17 race_rider.threshold_power 400 race_rider.max_power 1000 settings_r.fatigue_power_rate 1 Start Order [2,3,0,1] Instructions [[16,"drop=3"],[19,"drop=3"],[78,"effort=6.16"],[116,"drop=3"],[119,"drop=3"],[169,"effort=5.94"],[194,"drop=3"],[198,"drop=2"],[202,"effort=5.32"],[207,"drop=1"],[219,"drop=3"],[223,"effort=8.71"],[227,"effort=9.98"],[255,"drop=2"],[266,"effort=6.89"],[267,"effort=9.68"],[289,"effort=4.32"]]

doesn't seem to pick up the instructions?

-- ah, ok, good news is that it seems legit, that if a rider gets an effort 9 with no fatigue it builds up a head of steam and can only reduce power so fast, ends up clocking up a ton of fatigue.

-- so,  clarify again what the issue is... ah, the maximum 

-- oh dear, so max_performance_failure_percentage found 1.1843064351922996. what happens when this happens? 
-- shite,  target_power = target_power - (target_power*performance_failure_percentage); 
-- this can return a BIGGER value, not a smaller... performance_failure_percentage needs to be limited to < 1.
-- end up with NEGATIVE target power :-( I guess this just means the rider will start slowing down, and since they can only slow down by a certian rate this is very limited, and if they are just following they will essentially reset their effort in the next timestep?
-- huh, so the failure is actually constrained by the maximum power drop allowed per step... currently 40 
-- does this get weird tho if the LEADING rider fails? probably IS the leading rider accumuating this big level of fatigue?
-- how do we fix this and allow a rider to really fail in a seemingly organic way?
-- 1: allow the rider to slow by more than 40 watts in one go (e.g. 100)?
-- 2: maximum failure amount = 1, never > 1. This will make them slow by 100 and no more.

-- worth a try to see what happens
-- so the crux is where I use current_fatigue/current_fatigue_max and current_fatigue>current_fatigue_max.

-- simples fix: if (current_fatigue>current_fatigue_max) then current_fatigue_max = current_fatigue, so this will just max out this?

change "power_adjustment_step_size_down": -40
to
"power_adjustment_step_size_down": -100

-- results are... different. riders get dropped far more easily... step size down is probably too big in some cases? like are there cases where it hsould be lower or higher? try run 20 gens with no noise and see what happens

-- idea, use robustness to represent or influence  fitness. Here' how I explained the idea in an email to Colm> split the population into tournaments as usual.
> run each race and record its result (finish time, lower = better). But also, for each, run N other races, where for each one you add a mutation. So, for race 0 there is no mutation, for race 1 there is 1, and so on until race N includes N mutations (either accumulate the mutations or generate new ones each time). Record the result of each. The original genome is selected, not the mutated ones.
> return, as fitness, 0_mutations_result*x + (average_cost_of_each_additional_mutation)*y where x,y are [0,1] range so can weight the importance of each. I'd like to see what happens if you make the fitness purely based on the cost of mutations, might look strange! Could also try out a few other metrics other than average_cost_of_each_additional_mutation, or a different way to generate mutations that would be probing the landscape around a solution to try and find the slopes.

-- need to build a sequence that will trigger the testing of best_in_final_gen strategy for different noise levels and store the results: end goal is to make a 3d graph

x-axis: evolved fitness noise level
y-axis: test fitness noise level
z-axis: average speed of ten runs of best-in-gen from x in y  noise level

-- to test, try it with two main iterations.

{"iterations":2,"active":0,"variations":[{"iterations":[2,1],"type":"global","property":"enable_instruction_noise_1_random","values":[1,1]},{"iterations":[2,1],"type":"global","property":"noise_1_probability_instruction_misheard","values":[0.5,0]}],"best_in_final_gen_tests":[{"iterations":[11,10,9,8,7,6,5,4,3,2,1],"type":"global","property":"noise_1_probability_instruction_misheard","values":[1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0]}]}

what would we need to add to results?

in results, add a new struct; key thing is to be able to name/label each result correctly, i.e. the x,y,z of any graph

best_in_gen_tests: [{variation:[{type:"global","property":"noise_1_probability_instruction_misheard","value":1}],"reps":10,"test_result":276.45},{"noise_1_probability_instruction_misheard":0.9,"reps":10,"average_result":304.01}...]

-- where do i start? add them to the sequence, read them, then look at parsing?

-- not running the sequence. json is fierce frail

{"iterations":2,"active":1,"variations":[{"iterations":[2,1],"type":"global","property":"enable_instruction_noise_1_random","values":[1,1]},{"iterations":[2,1],"type":"global","property":"noise_1_probability_instruction_misheard","values":[0.5,0]}],"best_in_final_gen_tests":[{"iterations":[11,10,9,8,7,6,5,4,3,2,1],"type":"global","property":"noise_1_probability_instruction_misheard","values":[1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0]}],"experiments":[{"client_id":"","iteration":2,"status":"active"}]}


{
	"iterations": 2,
	"active": 1,
	"variations": [
		{
			"iterations": [
				2,
				1
			],
			"type": "global",
			"property": "enable_instruction_noise_1_random",
			"values": [
				1,
				1
			]
		},
		{
			"iterations": [
				2,
				1
			],
			"type": "global",
			"property": "noise_1_probability_instruction_misheard",
			"values": [
				0.5,
				0
			]
		}
	],
		"best_in_final_gen_tests": [
		{
			"iterations": 11,
			"repeat_each":10,
			"variations":[
			{
					"type": "global",
					"property": "noise_1_probability_instruction_misheard",
					"values": [
						1.0,
						0.9,
						0.8,
						0.7,
						0.6,
						0.5,
						0.4,
						0.3,
						0.2,
						0.1,
						0.0
					]
				}
			]
		}
	]
}


-- error with client_id again, since I am OFFLINE

-- prints the new settings in the sequence running part now, but need to send it to the GA... maybe could add it to the global settings object itself? wouldn't have to add any arg that way.

-- in the GA, need to 
IF LAST GEN
run games
IF best_in_final_gen_tests
get best-in-gen instructions
look for iterations, parse variations, for each, update the settings then run the race
package results and add to the main results data object 

-- finally it is saving new data... doesn't seem to have the last one (0.0). how many does it actually run?

-- doesn't add details for the 0.0 value I guess because 0 is falsy?

-- arg, thought i had it but i broke it :-( used 
  if(typeof v_details.values[current_iteration] === 'undefined'){
  instead of
    if(typeof v_details.values[current_iteration] !== 'undefined'){
	
	
-- need to email paper to 

DiscoveringTeamPursuitTrackCyclingStrategiesUsingAGeneticAlgorithm_DK_COR_2022


-- try to get the data back out from the mongo 

-- just want simple list of triples or three single lists 
-- lob it into the data_display textarea

-- results are in the last generation... should really be outside but howsover

-- monday 21st feb
-- do the results appear in the textarea?
-- yup, they look like this:
["[{\"variation\":[{\"type\":\"global\",\"property\":\"noise_1_probability_instruction_misheard\",\"value\":1}],\"reps\":10,\"test_result\":328.5271},{\"variation\":[{\"type\":\"global\",\"property\":\"noise_1_probability_instruction_misheard\",\"value\":0.9}],\"reps\":10,\"test_result\":319.44710000000003},{\"variation\":[{\"type\":\"global\",\"property\":\"noise_1_probability_instruction_misheard\",\"value\":0.8}],\"reps\":10,\"test_result\":298.1650000000001},{\"variation\":[{\"type\":\"global\",\"property\":\"noise_1_probability_instruction_misheard\",\"value\":0.7}],\"reps\":10,\"test_result\":320.9687},{\"variation\":[{\"type\":\"global\",\"property\":\"noise_1_probability_instruction_misheard\",\"value\":0.6}],\"reps\":10,\"test_result\":317.6809},{\"variation\":[{\"type\":\"global\",\"property\":\"noise_1_probability_instruction_misheard\",\"value\":0.5}],\"reps\":10,\"test_result\":325.3721},{\"variation\":[{\"type\":\"global\",\"property\":\"noise_1_probability_instruction_misheard\",\"value\":0.4}],\"reps\":10,\"test_result\":321.40500000000003},{\"variation\":[{\"type\":\"global\",\"property\":\"noise_1_probability_instruction_misheard\",\"value\":0.3}],\"reps\":10,\"test_result\":303.8983},{\"variation\":[{\"type\":\"global\",\"property\":\"noise_1_probability_instruction_misheard\",\"value\":0.2}],\"reps\":10,\"test_result\":292.4171999999999},{\"variation\":[{\"type\":\"global\",\"property\":\"noise_1_probability_instruction_misheard\",\"value\":0.1}],\"reps\":10,\"test_result\":293.97920000000005},{\"variation\":[{\"type\":\"global\",\"property\":\"noise_1_probability_instruction_misheard\",\"value\":0}],\"reps\":10,\"test_result\":288.00500000000005}]"]

-- example variation
{"variation":[{"type":"global","property":"noise_1_probability_instruction_misheard","value":1}],"reps":10,"test_result":328.5271}

-- see can i parrs this and instead produce simple values with the noise_1_probability_instruction_misheard and test_result... will then just need to add the noise_1_probability_instruction_misheard of the final-best-in-gen-race itself, and select multiple entries to create the 3d dataset.

--  nasty bug based on me not understanding how select case works in JS. fixed.

-- now it shows doubles, but need to add the third datapoint then be able to select multiple data results. how/where do i add this? return it from app.js? might need to extend the data that is sent back, and add on a best_in_final_gen_noise_value?

-- so now the data returned should have
	return_data.best_in_final_gen_noise_value = 0;
	return_data.data_rows = [.......];
	
-- Tuesday 22/2. need to run a bigger sequence 
{"iterations":11,"active":1,"variations":[{"iterations":[11,10,9,8,7,6,5,4,3,2,1],"type":"global","property":"enable_instruction_noise_1_random","values":[1,1,1,1,1,1,1,1,1,1,1]},{"iterations":[11,10,9,8,7,6,5,4,3,2,1],"type":"global","property":"noise_1_probability_instruction_misheard","values":[1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0]}],"best_in_final_gen_tests":[{"iterations":11,"repeat_each":20,"variations":[{"type":"global","property":"noise_1_probability_instruction_misheard","values":[1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0]}]}]}



-- have collected data for a few scenarions now, but the terrain is very mountainous... need to rerun one e.g. 3 strong and see if they vary a lot. if they vary a lot then it is struggling to find an optimum?

-- march 7th. can I fix the 3d graphs or even twig the problem?

-- do i need to re-run the 0-strong/2-strong/3-strong versions with the 21 data points instead of 11?

-- 3d graphs defo seem wrong, like it is picking up the wrong points- seems to be peaks at test noise = 0.65 or so, but this is not in the data.

-- hmmm, it does appear that when the ga noise is 0.65, the results are poor, i.e. always slow, much more so than when noise is 0.7

-- does this mean it simply happens to find a bad solution? does it struggle to find good solutions here or is it just chance.

-- rerun the experiment and see if the graph looks very different.

-- March 11th. Set up a few experiments to try find the fastest race for all weak... larger population, more generations, different mutation rates?

-- current default 
	"ga_population_size": 3000,
	"ga_population_size_first_generation": 3000,
	"ga_number_of_generations": 50,
	"ga_max_timestep": 450,
	"ga_probability_of_instruction_per_timestep_lower": 0.02,
	"ga_probability_of_instruction_per_timestep_upper": 0.04,
	"ga_probability_of_drop_instruction": 0.6,
	"ga_p_crossover": 0.3,
	"ga_crossover_length_adjustment_probability": 0.5,
	"crossover_apply_mutation_probability": 0.5
	
	"ga_p_shuffle_start": 0.003,
	"ga_p_add_instruction": 0.009,
	"ga_p_delete_instruction": 0.2,
	"ga_p_change_effort": 0.015,
	"ga_p_change_drop": 0.015,
	"ga_p_move_instruction": 0.015,
	"ga_range_to_move_instruction": 12,
	"ga_range_to_change_effort": 2.5,
	

let's try a bunch of these... make pop 5000, 50 gens fine, just tweeak some ga probs
-- first just see if the crossover choice matters>
	"ga_p_crossover": 0.3,


-- Monday 21st. March. Need to finish paper 2 this week. But what is the actual next task? 

ok, so the issue with the odd 0.65 times in 3 Strong seems to makse sense. The best-in-final-gen race finishes in a speedy 

instructions 
[[18,"effort=4.63"],[89,"drop=2"],[138,"drop=3"],[142,"drop=1"],[213,"drop=1"],[243,"effort=7.68"],[260,"effort=7.44"]]
This finishes in 269.894 seconds

how do i cite and image again in Overleaf? Like so: 

refer to figure \ref{fig:powerOutputFailure}

\begin{figure}[htp]
  \centering
  \includegraphics[width=1\textwidth]{failure_only_gen50_0_61701aefeeefc52d541b5a36_2021-9-20_14-44.png}
  \caption{Power Output for Winning Races in High-Failure Environment }
  \label{fig:powerOutputFailure}
\end{figure}##


-- need the graphs to look alike so will have to do all the graphs with Matplotlib- get it to export the data into the textarea for every graph?

Need to explain WHY the 0.6 levels are so much higher than the 0.7 ones. They require a very specific level of noise, i.e. to push UP the first effort instruction. 

-- run a bunch with 0.6/0.65/0.7 GA noise and tests?
{
	"iterations": 12,
	"active": 0,
	"variations": [
		{
			"iterations": [
				12,
				11,
				10,
				9,
				8,
				7,
				6,
				5,
				4,
				3,
				2,
				1
			],
			"type": "global",
			"property": "enable_instruction_noise_1_random",
			"values": [
				1,
				1,
				1,
				1,
				1,
				1,
				1,
				1,
				1,
				1,
				1,
				1
			]
		},
		{
			"iterations": [
				12,
				11,
				10,
				9,
				8,
				7,
				6,
				5,
				4,
				3,
				2,
				1
			],
			"type": "global",
			"property": "noise_1_probability_instruction_misheard",
			"values": [
				0.7,
				0.7,
				0.7,
				0.7,
				0.65,
				0.65,
				0.65,
				0.65,
				0.6,
				0.6,
				0.6,
				0.6
			]
		}
	],
	"best_in_final_gen_tests": [
		{
			"iterations": 21,
			"repeat_each": 20,
			"variations": [
				{
					"type": "global",
					"property": "noise_1_probability_instruction_misheard",
					"values": [
						1,
						0.95,
						0.9,
						0.85,
						0.8,
						0.75,
						0.7,
						0.65,
						0.6,
						0.55,
						0.5,
						0.45,
						0.4,
						0.35,
						0.3,
						0.25,
						0.2,
						0.15,
						0.1,
						0.05,
						0
					]
				}
			]
		}
	]
}


-- thursday 24th of March. Or is it May? September? 2055? did i ever exist? heat death of the universe be the backdrop for all that exist?

-- get the data and graph the 0.6/0.65/0.7 in matplotlib... then get the other diagram in matplotlib, then add graphs for the all weak/1 strong/2strong versions. do i have to run any of those again? could add a search and select-all to the UI?

-- hmmmm, looks like the sequence is ballsed up, the tests only ran up to 0.55. ah crap, I had iterations at 11 instead of 21. SHould have a UI way to re-run those manually :-( Would be slow to make though.

maybe add a tags entry in sequence and results that stores the sequence tags in the results ,which can then be searched? there are kinda too many results to just browse now. can then also add a select all button?

-- added a sequence_form_tags form textarea
-- more complex than i thought. Added resultTags to results form. 


-- added a graph...  but it is so so messy. I need to explain why particular strategies are so noise-intolerant... and then figure out why they are actually found by the GA.	

-- find the result for 0.6 and 0.7 to compare-- 0.6 62260d7a4ebddf4748185351 race url http://127.0.0.1:3003/tpgame.html?source=results&results_id=62260d7a4ebddf4748185351&startorder=1,2,0,3&instructions=%5B%5B18,%22effort=4.63%22%5D,%5B89,%22drop=2%22%5D,%5B138,%22drop=3%22%5D,%5B142,%22drop=1%22%5D,%5B213,%22drop=1%22%5D,%5B243,%22effort=7.68%22%5D,%5B260,%22effort=7.44%22%5D%5D&noise_alterations=%7B%2218%22:%7B%22original_instruction%22:%5B18,%22effort=4.63%22%5D,%22altered_instruction%22:%5B18,%22effort=6.151875784572508%22%5D,%22type%22:%22random_effort%22%7D,%22138%22:%7B%22original_instruction%22:%5B138,%22drop=3%22%5D,%22altered_instruction%22:%5B138,%22drop=2%22%5D,%22type%22:%22random_drop%22%7D,%22142%22:%7B%22original_instruction%22:%5B142,%22drop=1%22%5D,%22altered_instruction%22:%5B142,%22drop=3%22%5D,%22type%22:%22random_drop%22%7D,%22213%22:%7B%22original_instruction%22:%5B213,%22drop=1%22%5D,%22altered_instruction%22:%5B213,%22drop=3%22%5D,%22type%22:%22random_drop%22%7D,%22243%22:%7B%22original_instruction%22:%5B243,%22effort=7.68%22%5D,%22altered_instruction%22:%5B243,%22effort=5.887529149495269%22%5D,%22type%22:%22random_effort%22%7D,%22260%22:%7B%22original_instruction%22:%5B260,%22effort=7.44%22%5D,%22altered_instruction%22:%5B260,%22effort=9%22%5D,%22type%22:%22random_effort%22%7D%7D&performance_failures=%7B%7D

[[18,"effort=4.63"],[89,"drop=2"],[138,"drop=3"],[142,"drop=2"],[213,"drop=1"],[243,"effort=7.68"],[260,"effort=7.44"]]
[[18,"effort=6.151875784572508"],[89,"drop=2"],[138,"drop=2"],[142,"drop=3"],[213,"drop=3"],[243,"effort=5.887529149495269"],[260,"effort=9"]]

noise 
{"18":{"original_instruction":[18,"effort=4.63"],"altered_instruction":[18,"effort=6.151875784572508"],"type":"random_effort"},"138":{"original_instruction":[138,"drop=3"],"altered_instruction":[138,"drop=2"],"type":"random_drop"},"142":{"original_instruction":[142,"drop=1"],"altered_instruction":[142,"drop=3"],"type":"random_drop"},"213":{"original_instruction":[213,"drop=1"],"altered_instruction":[213,"drop=3"],"type":"random_drop"},"243":{"original_instruction":[243,"effort=7.68"],"altered_instruction":[243,"effort=5.887529149495269"],"type":"random_effort"},"260":{"original_instruction":[260,"effort=7.44"],"altered_instruction":[260,"effort=9"],"type":"random_effort"}}


-- fruiday 25th. mad slow progress. needs to 
-- maybe I can build the search tags button?
-- started this. need to get search term then actually filter the results. 
-- then need to finish the explanation of that noise section. then on on on.

-- mumber of instructions- is it falling as noise is added, i..e does it affect the tolerance?

-- Monday 4 4. let's try do 4 hours.

-- first build that dumb tag search?

-- l ooks like |I might need to create an index on the tags field 
e.g.
db.experiment_results.createIndex( { tags: "text", notes: "text" } )

how does this work then?

db.experiment_results.find( { $text: { $search: "\"3StrongA\"" } } )

seems to work... implement this so in the Node code

-- if term is blank show all...
-- also add a dedicated Show All button ?

-- it is actually searching the tags and the notes... improve the button text 

-- add a Select All button

-- make it a proper toggle, add a gobal var to keep track of the toggle state

-- ok, this now seems to work... do i need to be able to search the sequences?

-- limit the height of the sequences list

-- might as well add the search to the sequences as well, need to keep improving this.

-- nasty issue not reading the e object in the keypress. 

also, search brings back everything... 

-- error searching sequences:  err MongoError: text index required for $text query 
need to add the index for the sequences

db.experiment_results.createIndex( { tags: "text", notes: "text" } )

had to replace the d3 key handling with jquery.

-- next up, need to get the rawdata from the other graphs so that I can build them in all via Matplotlib

-- have some data in the raw box now... bug, it is missing the 1st entry except for the first selected row? warum?

-- loop was starting at 1 not 0 for some daft reason.

-- try to remake the graph in Overleafd using matplotlib 

-- looks like the original 0.6/.65/.7 data comes from the sequence 621fb189e07d6b17109622cf ?
result ids 
621fd5bd44647e2364004b2c
621fd4f944647e2364004b2b
621fd41644647e2364004b2a

Nop. It is sequence 621fb189e07d6b17109622cf
result ids
62260d7a4ebddf4748185351
62260ccf4ebddf4748185350
62260c224ebddf474818534f

ah wait, same sequecne id as I reused it. dumb dumb dumb.
actual data 
[[[0,273.795],[1,273.562],[2,272.581],[3,271.882],[4,271.414],[5,272.315],[6,271.97],[7,270.795],[8,270.733],[9,271.662],[10,270.479],[11,271.575],[12,271.786],[13,271.834],[14,270.284],[15,270.743],[16,270.284],[17,270.739],[18,270.936],[19,271.153],[20,271.72],[21,271.701],[22,270.538],[23,271.343],[24,270.918],[25,270.47],[26,269.091],[27,271.143],[28,270.782],[29,270.49],[30,271.478],[31,271.762],[32,270.795],[33,270.947],[34,270.442],[35,271.523],[36,271.758],[37,270.162],[38,270.462],[39,271.793],[40,271.611],[41,271.155],[42,270.453],[43,269.737],[44,271.519],[45,271.096],[46,270.582],[47,271.49],[48,270.245],[49,271.149]],[[0,273.795],[1,273.562],[2,272.581],[3,271.882],[4,271.414],[5,272.315],[6,271.97],[7,270.795],[8,270.733],[9,271.662],[10,270.479],[11,271.575],[12,271.786],[13,271.834],[14,270.284],[15,270.743],[16,270.284],[17,270.739],[18,270.936],[19,271.153],[20,271.72],[21,271.701],[22,270.538],[23,271.343],[24,270.918],[25,270.47],[26,269.091],[27,271.143],[28,270.782],[29,270.49],[30,271.478],[31,271.762],[32,270.795],[33,270.947],[34,270.442],[35,271.523],[36,271.758],[37,270.162],[38,270.462],[39,271.793],[40,271.611],[41,271.155],[42,270.453],[43,269.737],[44,271.519],[45,271.096],[46,270.582],[47,271.49],[48,270.245],[49,271.149]],[[0,273.795],[1,273.562],[2,272.581],[3,271.882],[4,271.414],[5,272.315],[6,271.97],[7,270.795],[8,270.733],[9,271.662],[10,270.479],[11,271.575],[12,271.786],[13,271.834],[14,270.284],[15,270.743],[16,270.284],[17,270.739],[18,270.936],[19,271.153],[20,271.72],[21,271.701],[22,270.538],[23,271.343],[24,270.918],[25,270.47],[26,269.091],[27,271.143],[28,270.782],[29,270.49],[30,271.478],[31,271.762],[32,270.795],[33,270.947],[34,270.442],[35,271.523],[36,271.758],[37,270.162],[38,270.462],[39,271.793],[40,271.611],[41,271.155],[42,270.453],[43,269.737],[44,271.519],[45,271.096],[46,270.582],[47,271.49],[48,270.245],[49,271.149]]]

-- moving python project to main repo... do I need to add stuff to the ignore file? Oh, this top level RESEARCH folder is not the git repo. 


-- need to add the result/sequence id to the search index for results/sequences 

-- first get the indexes 
db.experiment_results.getIndexes()

index name is tags_text_notes_text
delete this with 
db.experiment_results.dropIndex( "tags_text_notes_text" )
-- now recreate it with the id 
db.experiment_results.createIndex( { _id: "text", tags: "text", notes: "text" } )
-- new index has name _id_text_tags_text_notes_text

-- do the same for the sequences 
db.experiment_sequences.dropIndex( "tags_text_notes_text" )
db.experiment_sequences.createIndex( { _id: "text", tags: "text", notes: "text" } )
-- new index _id_text_tags_text_notes_text

-- do they work?
nope. seems to not search but the id :-( 

-- need to add an OR into the find()?
db.inventory.find( { $or: [ { quantity: { $lt: 20 } }, { price: 10 } ] } )

current 
  db.getDB().collection(collectionResults).find({ $text: { $search: "\""+ searchTerm +"\"" } }
new
  db.getDB().collection(collectionResults).find({ $or: [{ "_id" : ObjectId(searchTerm), "x" : 1 }, {$text: { $search: "\""+ searchTerm +"\"" }}] }
  
  nope, returns no results... test one in Robo3T I guess 
  ObjectId("623c75494c8f6617d4055de0")


-- can't match up the data in pycharm with the results now... what the fuck am i even doing?
-- 0.6 data should be either
result 62260d7a4ebddf4748185351  or 
result 621fd5bd44647e2364004b2c (older)

-- search fails if it is not an objectid format... need to improve it; check to see if the term is a valid objectid before using it 

const ObjectId = require('mongoose').Types.ObjectId;
  function isObjectIdValid(id) {    
  if (ObjectId.isValid(id)) {
  if (String(new ObjectId(id)) === id) { 
  return true      } else {        return false      }    } else {      return false    }  }

-- seems to work, need this for sequences, too?

ok. seems to work.

find result 62260d7a4ebddf4748185351 then find the other two next to it. generate graphs again

data for all three 
[[[0,273.795],[1,273.562],[2,272.581],[3,271.882],[4,271.414],[5,272.315],[6,271.97],[7,270.795],[8,270.733],[9,271.662],[10,270.479],[11,271.575],[12,271.786],[13,271.834],[14,270.284],[15,270.743],[16,270.284],[17,270.739],[18,270.936],[19,271.153],[20,271.72],[21,271.701],[22,270.538],[23,271.343],[24,270.918],[25,270.47],[26,269.091],[27,271.143],[28,270.782],[29,270.49],[30,271.478],[31,271.762],[32,270.795],[33,270.947],[34,270.442],[35,271.523],[36,271.758],[37,270.162],[38,270.462],[39,271.793],[40,271.611],[41,271.155],[42,270.453],[43,269.737],[44,271.519],[45,271.096],[46,270.582],[47,271.49],[48,270.245],[49,271.149]],

[[0,273.795],[1,273.562],[2,272.581],[3,271.882],[4,271.414],[5,272.315],[6,271.97],[7,270.795],[8,270.733],[9,271.662],[10,270.479],[11,271.575],[12,271.786],[13,271.834],[14,270.284],[15,270.743],[16,270.284],[17,270.739],[18,270.936],[19,271.153],[20,271.72],[21,271.701],[22,270.538],[23,271.343],[24,270.918],[25,270.47],[26,269.091],[27,271.143],[28,270.782],[29,270.49],[30,271.478],[31,271.762],[32,270.795],[33,270.947],[34,270.442],[35,271.523],[36,271.758],[37,270.162],[38,270.462],[39,271.793],[40,271.611],[41,271.155],[42,270.453],[43,269.737],[44,271.519],[45,271.096],[46,270.582],[47,271.49],[48,270.245],[49,271.149]],

[[0,273.795],[1,273.562],[2,272.581],[3,271.882],[4,271.414],[5,272.315],[6,271.97],[7,270.795],[8,270.733],[9,271.662],[10,270.479],[11,271.575],[12,271.786],[13,271.834],[14,270.284],[15,270.743],[16,270.284],[17,270.739],[18,270.936],[19,271.153],[20,271.72],[21,271.701],[22,270.538],[23,271.343],[24,270.918],[25,270.47],[26,269.091],[27,271.143],[28,270.782],[29,270.49],[30,271.478],[31,271.762],[32,270.795],[33,270.947],[34,270.442],[35,271.523],[36,271.758],[37,270.162],[38,270.462],[39,271.793],[40,271.611],[41,271.155],[42,270.453],[43,269.737],[44,271.519],[45,271.096],[46,270.582],[47,271.49],[48,270.245],[49,271.149]]]

data for .6 only 
[[[0,273.303],[1,271.672],[2,272.673],[3,272.284],[4,270.447],[5,270.235],[6,270.447],[7,270.25],[8,271.334],[9,270.524],[10,270.605],[11,271.094],[12,269.514],[13,271.382],[14,270.842],[15,271.177],[16,269.435],[17,271.439],[18,271.181],[19,270.559],[20,269.901],[21,268.999],[22,270.681],[23,271.246],[24,270.034],[25,269.679],[26,269.747],[27,270.02],[28,270.337],[29,270.988],[30,268.619],[31,270.875],[32,269.7],[33,271.307],[34,269.678],[35,270.746],[36,270.951],[37,270.392],[38,271.12],[39,270.754],[40,269.852],[41,270.903],[42,270.749],[43,269.99],[44,270.268],[45,270.537],[46,271.066],[47,270.9],[48,271.138],[49,269.894]]]

NOT the same... what's a goin on? 

ahhh, all three are the same... muck.
-- hadn't updated an index, all were at 0. eejit.

[[[0,273.795],[1,273.562],[2,272.581],[3,271.882],[4,271.414],[5,272.315],[6,271.97],[7,270.795],[8,270.733],[9,271.662],[10,270.479],[11,271.575],[12,271.786],[13,271.834],[14,270.284],[15,270.743],[16,270.284],[17,270.739],[18,270.936],[19,271.153],[20,271.72],[21,271.701],[22,270.538],[23,271.343],[24,270.918],[25,270.47],[26,269.091],[27,271.143],[28,270.782],[29,270.49],[30,271.478],[31,271.762],[32,270.795],[33,270.947],[34,270.442],[35,271.523],[36,271.758],[37,270.162],[38,270.462],[39,271.793],[40,271.611],[41,271.155],[42,270.453],[43,269.737],[44,271.519],[45,271.096],[46,270.582],[47,271.49],[48,270.245],[49,271.149]],[[0,273.688],[1,270.175],[2,272.986],[3,269.272],[4,270.74],[5,271.639],[6,272.148],[7,271.923],[8,271.212],[9,271.212],[10,271.666],[11,271.289],[12,270.472],[13,270.472],[14,270.398],[15,270.472],[16,270.269],[17,270.055],[18,270.281],[19,269.113],[20,270.298],[21,271.011],[22,271.097],[23,270.602],[24,271.51],[25,271.365],[26,270.587],[27,271.807],[28,270.58],[29,271.519],[30,269.526],[31,269.548],[32,271.747],[33,271.372],[34,270.842],[35,270.753],[36,269.513],[37,270.735],[38,269.851],[39,270.992],[40,270.314],[41,270.403],[42,270.403],[43,269.964],[44,270.838],[45,270.915],[46,270.636],[47,269.129],[48,270.307],[49,270.065]],[[0,273.303],[1,271.672],[2,272.673],[3,272.284],[4,270.447],[5,270.235],[6,270.447],[7,270.25],[8,271.334],[9,270.524],[10,270.605],[11,271.094],[12,269.514],[13,271.382],[14,270.842],[15,271.177],[16,269.435],[17,271.439],[18,271.181],[19,270.559],[20,269.901],[21,268.999],[22,270.681],[23,271.246],[24,270.034],[25,269.679],[26,269.747],[27,270.02],[28,270.337],[29,270.988],[30,268.619],[31,270.875],[32,269.7],[33,271.307],[34,269.678],[35,270.746],[36,270.951],[37,270.392],[38,271.12],[39,270.754],[40,269.852],[41,270.903],[42,270.749],[43,269.99],[44,270.268],[45,270.537],[46,271.066],[47,270.9],[48,271.138],[49,269.894]]]

note that the ordering is REVERSED: 0.6 data at the END.

put new graph in paper... should switch the ordering, to match the previous one.

-- tuesday 5th april.

-- commit changes to git and push to github. need a new key?
-- seemd to work ok. Python graph files are not in git though.

-- do the number of instructions in strategies fall as noise is increased?


-- maybe run that 0.6 race and show how changes to that first noise instruction cause major slowdown?



4, 309.336
4.1, 307.52
4.2, 305.52
4.3, 303.553
4.4, 301.378
4.5, 299.472
4.6, 297.598
4.7, 295.75
4.8, 293.931
4.9, 292.13
5, 290.616
5.1, 288.874
5.2, 287.156
5.3, 285.459 
5.4, 283.787
5.5, 281.872
5.6, 280.248
5.7, 278.646
5.8, 277.108
5.9, 275.66
6, 274.318
6.1, 271.403
6.12, 270.785
6.14, 270.19,
6.15, 269.951
6.151875784572508, 269.894
6.16, 269.651
6.165, 269.5
6.166, 269.471,
6.1663, 269.462
6.1664, 271.143
6.1665, 271.14
6.167, 271.125
6.168, 271.095
6.17, 273.368
6.18, 296.101
6.19,  296.744
6.2, 297.206
6.3, 300.738
6.4, 302.509
6.5, 303.444
6.6, 303.806
6.7, 304.263
6.8, 304.542
6.9, 304.586
7, 305.001 
7.1, 304.784
7.2, 304.932
7.3, 305.128
7.4, 304.976
7.5, 304.82
7.6, 305.078
8, 304.596
8.5, 304.529
9, 304.529

-- as a list for Python
[[4, 309.336],[4.1, 307.52],[4.2, 305.52],[4.3, 303.553],[4.4, 301.378],[4.5, 299.472],[4.6, 297.598],[4.7, 295.75],[4.8, 293.931],[4.9, 292.13],[5, 290.616],[5.1, 288.874],[5.2, 287.156],[5.3, 285.459 ],[5.4, 283.787],[5.5, 281.872],[5.6, 280.248],[5.7, 278.646],[5.8, 277.108],[5.9, 275.66],[6, 274.318],[6.1, 271.403],[6.12, 270.785],[6.14, 270.19,],[6.15, 269.951],[6.151875784572508, 269.894],[6.16, 269.651],[6.165, 269.5],[6.166, 269.471,],[6.1663, 269.462],[6.1664, 271.143],[6.1665, 271.14],[6.167, 271.125],[6.168, 271.095],[6.17, 273.368],[6.18, 296.101],[6.19,  296.744],[6.2, 297.206],[6.3, 300.738],[6.4, 302.509],[6.5, 303.444],[6.6, 303.806],[6.7, 304.263],[6.8, 304.542],[6.9, 304.586],[7, 305.001 ],[7.1, 304.784],[7.2, 304.932],[7.3, 305.128],[7.4, 304.976],[7.5, 304.82],[7.6, 305.078],[8, 304.596],[8.5, 304.529],[9, 304.529]]

-- wednesday 6th

-- can i know if that final best-in-gen strategy was produced by a mutation?

-- fiind that result in robo3t?
db.getCollection('experiment_results').find({ "_id" : ObjectId("62260d7a4ebddf4748185351")})

-- nope, can't really read much into it, other than that it looks like there is a lack of covergabce and a lot of variability in the best-in-generation strategies. How could I measure this variability within a population?

-- thursday 07
-- abandoned, wedding prepartion

-- mundee 11th, left home after lunch, still haven't done anything. trying to finish The Rings of Saturn. But what am I doing, really?

get the saving of average instruction length to work.-- huh, it appears to already be there?
ObjectId("623c75494c8f6617d4055de0")
-- yup, appears to be there... so let's try draw a matplotlib chart that compares a few?
-- need to first put data in the raw box?

instructions_change_per_generation
  raw_data = [];
  if(raw_data){
      $("#data_display").val(JSON.stringify(raw_data));
  }
  
-- sample output of raw data - note the null values for generation 49 in most of them
[[[0,13.565666666666667],[1,10.429],[2,8.554333333333334],[3,7.506],[4,6.547],[5,6.185],[6,6.041666666666667],[7,6.045],[8,5.934666666666667],[9,5.924333333333333],[10,5.848333333333334],[11,5.677666666666667],[12,5.458666666666667],[13,5.36],[14,5.225666666666666],[15,5.255],[16,5.229],[17,5.191666666666666],[18,5.157333333333334],[19,5.143666666666666],[20,5.095],[21,5.057333333333333],[22,5.049333333333333],[23,5.092],[24,5.179666666666667],[25,5.121666666666667],[26,5.045],[27,5.062666666666667],[28,5.101],[29,4.9943333333333335],[30,5.007],[31,5.084666666666666],[32,5.123333333333333],[33,5.220666666666666],[34,5.254666666666667],[35,5.331333333333333],[36,5.37],[37,5.346],[38,5.384333333333333],[39,5.300333333333334],[40,5.413],[41,5.584333333333333],[42,5.708],[43,5.648],[44,5.659666666666666],[45,5.699666666666666],[46,5.697333333333333],[47,5.721],[48,5.774],[49,5.686]],[[0,1.5746666666666667],[1,1.4596666666666667],[2,1.5293333333333334],[3,1.4756666666666667],[4,1.4676666666666667],[5,1.4596666666666667],[6,1.5063333333333333],[7,1.462],[8,1.4156666666666666],[9,1.4803333333333333],[10,1.4853333333333334],[11,1.4716666666666667],[12,1.442],[13,1.451],[14,1.4786666666666666],[15,1.4623333333333333],[16,1.4846666666666666],[17,1.474],[18,1.481],[19,1.4033333333333333],[20,1.4486666666666668],[21,1.4643333333333333],[22,1.4593333333333334],[23,1.5],[24,1.432],[25,1.468],[26,1.436],[27,1.411],[28,1.4156666666666666],[29,1.45],[30,1.5126666666666666],[31,1.4346666666666668],[32,1.4916666666666667],[33,1.4553333333333334],[34,1.5666666666666667],[35,1.4476666666666667],[36,1.445],[37,1.4406666666666668],[38,1.4643333333333333],[39,1.4543333333333333],[40,1.454],[41,1.487],[42,1.4516666666666667],[43,1.3753333333333333],[44,1.458],[45,1.477],[46,1.4493333333333334],[47,1.4596666666666667],[48,1.4666666666666666],[49,null]],[[0,0.933],[1,0.7706666666666667],[2,0.7396666666666667],[3,0.618],[4,0.591],[5,0.5546666666666666],[6,0.5743333333333334],[7,0.5566666666666666],[8,0.566],[9,0.5363333333333333],[10,0.4866666666666667],[11,0.5056666666666667],[12,0.4643333333333333],[13,0.4666666666666667],[14,0.4493333333333333],[15,0.478],[16,0.45366666666666666],[17,0.444],[18,0.428],[19,0.43566666666666665],[20,0.421],[21,0.476],[22,0.44666666666666666],[23,0.44233333333333336],[24,0.413],[25,0.43833333333333335],[26,0.43333333333333335],[27,0.42033333333333334],[28,0.43],[29,0.4206666666666667],[30,0.4593333333333333],[31,0.43766666666666665],[32,0.4593333333333333],[33,0.45166666666666666],[34,0.456],[35,0.4603333333333333],[36,0.48033333333333333],[37,0.47],[38,0.48533333333333334],[39,0.524],[40,0.512],[41,0.513],[42,0.5093333333333333],[43,0.521],[44,0.5116666666666667],[45,0.49633333333333335],[46,0.519],[47,0.49766666666666665],[48,0.531],[49,null]],[[0,3.3573333333333335],[1,3.0343333333333335],[2,3.131666666666667],[3,2.8033333333333332],[4,2.652],[5,2.5873333333333335],[6,2.545],[7,2.486],[8,2.3513333333333333],[9,2.3423333333333334],[10,2.221],[11,2.071],[12,2.0083333333333333],[13,1.9943333333333333],[14,1.9773333333333334],[15,1.923],[16,1.982],[17,1.9],[18,1.8586666666666667],[19,1.8806666666666667],[20,1.8356666666666666],[21,1.929],[22,1.8616666666666666],[23,1.973],[24,1.8713333333333333],[25,1.8773333333333333],[26,1.8663333333333334],[27,1.843],[28,1.849],[29,1.8416666666666666],[30,1.9786666666666666],[31,1.9646666666666666],[32,2.06],[33,2.009333333333333],[34,2.127666666666667],[35,2.070333333333333],[36,2.119666666666667],[37,2.0946666666666665],[38,2.0103333333333335],[39,2.042],[40,2.0616666666666665],[41,2.088],[42,2.071],[43,2.066],[44,2.1606666666666667],[45,2.096],[46,2.121],[47,2.1513333333333335],[48,2.1326666666666667],[49,null]]]

-- the ones that use an average calculation are null... 
doesn't create a new popualtion in the last generation, and that's where the data comes from... so need to ignore the last gen.

--  null causes a wobbly in Python, throws NameError: name 'null' is not defined
-- don't add them in the JS

-- new version of raw data, example 
[[[0,13.565666666666667],[1,10.429],[2,8.554333333333334],[3,7.506],[4,6.547],[5,6.185],[6,6.041666666666667],[7,6.045],[8,5.934666666666667],[9,5.924333333333333],[10,5.848333333333334],[11,5.677666666666667],[12,5.458666666666667],[13,5.36],[14,5.225666666666666],[15,5.255],[16,5.229],[17,5.191666666666666],[18,5.157333333333334],[19,5.143666666666666],[20,5.095],[21,5.057333333333333],[22,5.049333333333333],[23,5.092],[24,5.179666666666667],[25,5.121666666666667],[26,5.045],[27,5.062666666666667],[28,5.101],[29,4.9943333333333335],[30,5.007],[31,5.084666666666666],[32,5.123333333333333],[33,5.220666666666666],[34,5.254666666666667],[35,5.331333333333333],[36,5.37],[37,5.346],[38,5.384333333333333],[39,5.300333333333334],[40,5.413],[41,5.584333333333333],[42,5.708],[43,5.648],[44,5.659666666666666],[45,5.699666666666666],[46,5.697333333333333],[47,5.721],[48,5.774],[49,5.686]],[[0,1.5746666666666667],[1,1.4596666666666667],[2,1.5293333333333334],[3,1.4756666666666667],[4,1.4676666666666667],[5,1.4596666666666667],[6,1.5063333333333333],[7,1.462],[8,1.4156666666666666],[9,1.4803333333333333],[10,1.4853333333333334],[11,1.4716666666666667],[12,1.442],[13,1.451],[14,1.4786666666666666],[15,1.4623333333333333],[16,1.4846666666666666],[17,1.474],[18,1.481],[19,1.4033333333333333],[20,1.4486666666666668],[21,1.4643333333333333],[22,1.4593333333333334],[23,1.5],[24,1.432],[25,1.468],[26,1.436],[27,1.411],[28,1.4156666666666666],[29,1.45],[30,1.5126666666666666],[31,1.4346666666666668],[32,1.4916666666666667],[33,1.4553333333333334],[34,1.5666666666666667],[35,1.4476666666666667],[36,1.445],[37,1.4406666666666668],[38,1.4643333333333333],[39,1.4543333333333333],[40,1.454],[41,1.487],[42,1.4516666666666667],[43,1.3753333333333333],[44,1.458],[45,1.477],[46,1.4493333333333334],[47,1.4596666666666667],[48,1.4666666666666666]],[[0,0.933],[1,0.7706666666666667],[2,0.7396666666666667],[3,0.618],[4,0.591],[5,0.5546666666666666],[6,0.5743333333333334],[7,0.5566666666666666],[8,0.566],[9,0.5363333333333333],[10,0.4866666666666667],[11,0.5056666666666667],[12,0.4643333333333333],[13,0.4666666666666667],[14,0.4493333333333333],[15,0.478],[16,0.45366666666666666],[17,0.444],[18,0.428],[19,0.43566666666666665],[20,0.421],[21,0.476],[22,0.44666666666666666],[23,0.44233333333333336],[24,0.413],[25,0.43833333333333335],[26,0.43333333333333335],[27,0.42033333333333334],[28,0.43],[29,0.4206666666666667],[30,0.4593333333333333],[31,0.43766666666666665],[32,0.4593333333333333],[33,0.45166666666666666],[34,0.456],[35,0.4603333333333333],[36,0.48033333333333333],[37,0.47],[38,0.48533333333333334],[39,0.524],[40,0.512],[41,0.513],[42,0.5093333333333333],[43,0.521],[44,0.5116666666666667],[45,0.49633333333333335],[46,0.519],[47,0.49766666666666665],[48,0.531]],[[0,3.3573333333333335],[1,3.0343333333333335],[2,3.131666666666667],[3,2.8033333333333332],[4,2.652],[5,2.5873333333333335],[6,2.545],[7,2.486],[8,2.3513333333333333],[9,2.3423333333333334],[10,2.221],[11,2.071],[12,2.0083333333333333],[13,1.9943333333333333],[14,1.9773333333333334],[15,1.923],[16,1.982],[17,1.9],[18,1.8586666666666667],[19,1.8806666666666667],[20,1.8356666666666666],[21,1.929],[22,1.8616666666666666],[23,1.973],[24,1.8713333333333333],[25,1.8773333333333333],[26,1.8663333333333334],[27,1.843],[28,1.849],[29,1.8416666666666666],[30,1.9786666666666666],[31,1.9646666666666666],[32,2.06],[33,2.009333333333333],[34,2.127666666666667],[35,2.070333333333333],[36,2.119666666666667],[37,2.0946666666666665],[38,2.0103333333333335],[39,2.042],[40,2.0616666666666665],[41,2.088],[42,2.071],[43,2.066],[44,2.1606666666666667],[45,2.096],[46,2.121],[47,2.1513333333333335],[48,2.1326666666666667]]]

-- result 3-strong noise 0 noise 622616424ebddf474818535d 
-- result 3-strong noise 0.2 noise 6226134a4ebddf4748185359  
-- result 3-strong noise 0.3 noise 622611d14ebddf4748185357   
-- result 3-strong noise 0.5 noise 62260ee04ebddf4748185353  
-- result 3-strong noise  1 noise 622608224ebddf4748185349  

-- ok, so have the data in PyCHarm, need to draw 4 or 5 lines on the graph, export, and put into Overleaf with a sentence or two.

-- ok, so I want to create a heatmap that will provide more insight into how the instrucitons are set out: like an average of all strats in the population, a visual measure of diversity?

so can't seem to find how to create the heatmap I had in mind, but could just try with a scatter plot for now and see how I gets on
-- first thing is to be able to mark a generation for saving the data, then create data structures for it... defo a bit of work but hey, might be a nice graph.
-- added setting to global 
   "log_generation_instructions_info":[0,49]
  -- read this in the ga where you are saving and returning data?
  -- note we are not looking at the start order at all.... should we look at it? bloody complexity :-( 
  -- note this is genotype and no noise will be considered.
  -- how do we parse the instructions again?
    let inst = new_instructions[i][1].split("=");
	if (inst.length=2){
	  if(inst[0]=="effort"){
                let effort_value = parseFloat(inst[1]);
	  else if(inst[0]=="drop"){
                let drop_value  = parseInt(inst[1]);
-- what exactly does an instruction look like again? 
[122,"drop=3"] and [123,"effort=7.21"]

-- kinda starting to work... getting extra values in the drop column stuff... like [0,3,7,null]... will IF those out, but how are they even in there?

-- seems to be stopping at some point, nothing for the higher instructions at all... need to do some logging I guess, or debugger?

-- ok, seems to be improving. weird, even with only ten gens there seems to be rapid convergence towards the same instructions. 
-- next I need to push these into the results and check that they get stored in that generations results... could make it more efficient by making it sparse, i.e. only adding the objects as needed... 

-- here's generation 0 
{"effort":{"0":[3.07,7.78,4.57],"1":[2.81],"2":[3.64,4.66],"3":[3.38,7.79],"5":[4.82],"6":[7.21,3.7,9.94],"7":[6.73,9.54],"8":[1.06,4.52,1.6400000000000001,5.79],"9":[7.47,9.18,2.49,1.79],"10":[1.17,4.26,3.43,3.6],"11":[6.88],"13":[3.21],"14":[7.14,5.98,2.88],"15":[9.56,2.2199999999999998,9.88,1.69,2.56],"16":[3.48,4.95],"17":[9.3,1.74,3.01,9.22,7.43],"18":[8.75],"19":[4.08,3.45,6.07,6.21],"20":[4.07],"21":[4.91,8.04,8.08,1.56],"22":[3.38],"23":[3.16,3.08,8.1,6.73,7.45],"24":[6.14],"26":[6.99,1.6400000000000001,4.17],"27":[9.96,1.18,6.72],"28":[8.21,3.88,4.71,3.23],"30":[9.31,7.84,4.68],"31":[4.6899999999999995,2.4,7.24],"32":[2.01,7.74],"33":[5.83,9.3],"34":[6.93,6.8,1.92],"35":[5],"36":[6.75],"37":[3.69,6.99,6.34,7.24],"38":[7.22,5.72],"39":[9.76,8.32,3.11,7.43],"40":[8.719999999999999,2.96,7.75],"41":[8.58],"42":[9.98],"43":[6.89],"44":[5.01,4.2,9.31,2.06],"45":[5.27],"47":[1.63],"49":[6.6,9.61,5.91,7.26,5.57],"50":[6.62,6.26,4.85],"51":[7.41,3.59,3.84,8.45],"52":[5.91,9.31,3.09],"53":[8.95],"54":[9.69,5.92,5.4],"55":[5.24],"56":[7.15,5.7,1.5899999999999999],"58":[3.16,5.13,5.55],"60":[5.75,7.96],"61":[5.22,7.85,8.24],"62":[7.54,1.05,5.04],"63":[7.23,8.98,9.82,4.58],"64":[6.63,7.13,6.36,2.37],"65":[2.45,1.6400000000000001],"66":[6.23,9.75,4.279999999999999,5.31],"67":[4.17,1.4,3.42],"69":[9.15,9.69],"70":[3.78],"71":[4.17,4.109999999999999],"72":[1.13],"73":[7.38,2.35],"74":[6.36],"75":[4.07],"76":[5.03,8.27],"77":[8.55,1.87],"78":[2.79,9.16],"79":[6.09,2.8899999999999997,7.25],"81":[9.85],"82":[4.84],"83":[5.09,3.74,1.77],"84":[2.08],"85":[7.1,6.29,9.78,5.28],"86":[2.0700000000000003],"87":[2.81],"88":[3.75],"89":[7.96],"91":[3.13,9.52,1.3,6.67,9.6],"92":[4.720000000000001],"93":[2.19],"94":[2.65],"95":[3.14,4.48,2.05],"96":[8.55,8.1],"97":[4.21],"98":[5.26,7.24],"99":[8.08,1.42],"101":[8.48],"103":[9.68],"104":[8.04],"105":[7.59,7.86,9.79],"106":[5.36,9.7,7.25,6.1],"107":[3.71,6.22,8.54,8.5],"108":[9.57,1.76,8.129999999999999,6.32],"109":[6.73,1.54,9.68,8.24],"110":[2.92,6.23],"111":[7.19,8.01],"112":[6.14],"114":[1.32,1.81],"115":[6.71,7.05],"116":[4.85,8.15,4.74],"117":[2.7800000000000002],"118":[5.9],"119":[4.13,4.24,1.52,6.15],"120":[9.44,1.79,1.8],"121":[8.61,9.99,8.120000000000001,1.99],"122":[6.57,2.5700000000000003,4.51],"123":[2.12,6.63],"124":[9.17,5.1,8.43],"125":[2.52,3.13],"127":[3.81,3.17,7.91,8.64,4.35],"128":[2.79,6.64],"129":[2.62,6.58,8.719999999999999],"130":[8.66],"131":[6.24],"132":[3.44,2.2199999999999998,7.3],"133":[4.58],"134":[7.66,6.87,8.43,6.14,1.43,6.78],"135":[4.48,4.67,3.99],"136":[4.98,1.71,5.19],"137":[9.64,5.97],"138":[5.4,8.309999999999999,3.61,5.94],"140":[8.559999999999999,7.16],"142":[3.87,7.35,6.92,6.54,9.09],"143":[9.67],"144":[3.07],"145":[4.859999999999999],"146":[9.11,2.91,9.99,6.84],"147":[7.93,4.9399999999999995,2.34],"148":[6.06,5.74],"149":[1.96,7.63],"150":[1.85],"152":[2.51,6.48],"153":[7.19,9.9,1.35],"155":[2.1100000000000003,9.67],"156":[7.32,9.19],"157":[8.67],"158":[9.4,8.35,6.84],"159":[8.969999999999999,4.42],"160":[7.98,6.41,3.9],"161":[2.91,2.2199999999999998],"163":[5.81],"164":[6.24],"165":[4.9,4.859999999999999],"166":[3.65,6.76,8],"167":[5.61,6.82],"168":[4.96],"169":[8.55,8.809999999999999,1.79,5.23,1.95],"170":[6.11],"171":[6.49],"173":[8.17,1.22,6.81,6.03],"174":[1.29,1.95],"175":[2.79,6.31],"176":[9.8],"177":[5.15,7.69],"178":[4.25,6.99,2.1799999999999997],"180":[9.54],"181":[7.96],"182":[2.16,9.06],"183":[7.62,3.34,6.36,3.94,6.91],"184":[9.11,2.23],"185":[2.49,3.16],"186":[7.77,1.1,5.29],"187":[8.21,5.68],"188":[3.62,4.7,9.22,1.87],"189":[8.98,2.0700000000000003,6.21],"190":[2.04,7.68],"191":[4.71,1.18],"192":[1.33,8.75,5.95],"193":[6.08,8.83],"195":[5.82,2.2800000000000002,8.66,2.96],"196":[1.16],"197":[7.62],"199":[5.01],"200":[8.95,4.74],"201":[7.75,7.71,6.56],"202":[6.28,2.6100000000000003],"203":[7.73,5.49,4.37],"204":[3.06,9.78],"205":[3.97,4.109999999999999,6.01,6.15,3.41],"206":[9.59,5.67,7.19,7.89],"208":[7.93,5.7,6.56],"210":[8.45],"211":[7.66,5.33],"212":[8.17,4.87,5.72,5.63],"214":[6.21,6.81,3.74,6.52],"215":[4.23,4.84,6.47],"216":[9.52,8.7],"217":[2.19,3.83,6.11,9.12,7.59],"218":[6.31,2.19],"219":[8.690000000000001,7.39,1.34],"220":[6.38],"221":[1.1400000000000001,8.48,1.21,5.55],"222":[7.87],"223":[4.9],"224":[1.52,8.11,1.04],"225":[7.38,5.25,7.55],"227":[7.05],"228":[6.37,8.41,5.68],"229":[2.34],"230":[6.15],"231":[8.98,6.34],"232":[9.41,5.11],"234":[6.52],"235":[3.34,2.25],"236":[9.95],"237":[6.74,3.69,7.36],"238":[2.7,5.22],"240":[6.27],"241":[8.690000000000001,1.46,3.05],"243":[6.12,8.620000000000001],"244":[4.66],"245":[3.05],"246":[5.57],"247":[1.58],"248":[2.98],"249":[9.01,1.46,8.11],"251":[5.72,4.45],"252":[3.45,2.63,8.08],"253":[2.8200000000000003,3.24],"254":[5.66,8.73],"255":[4.27,7.82,3.46,5.62],"256":[8.36,9.65,9.14,5.09,8.719999999999999],"257":[2.2199999999999998,4.79,3.25],"258":[5.78,6.31],"259":[3.75,5.69,3.05],"260":[8.67,3.58,7.64],"262":[8.629999999999999,8.059999999999999],"263":[9.49],"264":[2.75],"265":[4.93],"266":[7.31,4.59,5.1],"267":[6.75,3.72,3.25],"268":[1.2,4.8],"269":[4.76,3.07,2.1,5.7],"270":[1.52],"271":[8.91,1.98,4.9399999999999995],"272":[2.27,9.46,8.49],"274":[5.52,6.29,9.54],"275":[6.4],"278":[4.34,9.34],"279":[7.77,8.14,5.71],"280":[8.280000000000001,7.4,7.84],"281":[9.67],"282":[1.12,8.030000000000001,5.33],"283":[6.12],"284":[1.96],"285":[6.71,7.28,8.120000000000001],"286":[9.28,5.33,9.49],"288":[2.02],"290":[2.79,9.14],"291":[7.83,5.13,5.83],"293":[3.68,5.51,3.3,5.09],"294":[6.38,5.31],"295":[7.81,4.46],"296":[3.59,8.940000000000001],"297":[9.69,7.85,4.02],"298":[6.78,3.33],"299":[1.77],"301":[1.51],"302":[2.59,1.6,1.56],"303":[8.74,4.720000000000001],"304":[2.29,2.74,4.91],"305":[6.36,7.77],"307":[9.02,1.8399999999999999,8.61],"308":[7.13],"309":[8.82],"310":[8.530000000000001],"313":[9.75],"314":[8.719999999999999,8.23],"315":[3.2],"316":[9.04,6.8,3.3],"317":[9.61,4.55],"318":[9.45,1.58],"320":[2.01],"321":[6.57,2.6799999999999997,4.79,8.15],"322":[2.99],"323":[4.2,1.18,3.77],"324":[2.75],"325":[2.81,6.36],"326":[7.48,2.24],"327":[4.27],"329":[2.3899999999999997,9.22,8.940000000000001,2.04,4.93,1.6600000000000001,9.99],"331":[4.66,8.74,8.02],"332":[4.92,4.5600000000000005],"333":[4.54],"334":[6.74],"335":[8.29,8.370000000000001],"336":[6.7],"337":[1.46,6.06,9.33,1.65,1.8199999999999998],"338":[9.16,8.6],"339":[4.52],"340":[3.53,8.690000000000001,1.35,3.67],"341":[7.79,5.34,1.3,2.04,5.32],"342":[5.33,7.65],"343":[9.42,2.45],"344":[5.05],"345":[9.2,6.41,5.36,4.58],"347":[3.96,7.53],"348":[5.17],"349":[1.67,8.93],"350":[5.97],"351":[3.86,4.140000000000001,7.64,5.72],"352":[7.28,6.03],"353":[8.8,3.26],"354":[5.86,7.37,7.25],"355":[1.16],"356":[3.41,7.72,4.029999999999999,6.59],"357":[4.12,1.17],"358":[8.59,6.65,4.3100000000000005,7.23],"359":[7.2],"360":[3.6,8.51],"362":[8.3,1.04,5.15,6.01],"363":[2.2,4.98,1.06,6.94,5.62],"364":[4.2,3.58],"365":[2.9],"366":[9.78],"367":[1.46],"368":[4.7,6.86,9.84],"369":[8.01,9.38],"371":[4.52,1.78],"372":[7.54],"373":[2.29],"374":[2.33,3.4,1.27],"375":[7.32,6.76,7.91,9.95,7.41],"376":[5.91,3.72],"377":[9.74,6.44],"378":[1.07,3.22],"380":[6.12,7.95,7.3],"381":[4.5,1.02],"382":[4.96,9.01,3.26,5.09],"383":[3.38],"385":[5.48,6.45],"386":[5.84,5.12],"387":[9.03,4.1,1.23],"388":[2.25,5.07],"389":[9.99],"390":[1.25],"393":[3.96,4.640000000000001],"394":[7.7,1.18],"395":[8.77],"397":[7.44],"398":[2.99],"399":[9.57],"400":[7.81,3.92,9.98],"401":[1.32,6.31,3.95],"402":[1.23,7.88,1.42,9.22,5.24,2.87],"403":[2.25],"405":[8.280000000000001],"406":[5.23],"407":[9.41,7.65],"409":[4.25,4.23],"410":[6.7,6.18],"411":[7.5,9.97],"412":[3.04,1.17,6.09],"413":[2.66,6.88,4.9,8.809999999999999,4.74],"414":[5.01],"415":[1.96],"416":[4.99],"417":[2.63,8.71],"418":[8.620000000000001],"419":[2.4],"421":[6.81],"422":[1.46],"423":[7.77],"424":[1.24,3.22,7.64,8.6,5.71],"425":[8.809999999999999,2.84,4.17,1.01],"427":[6.45],"430":[6.45,7.64],"431":[6.3,8.809999999999999,2.62,7.84],"432":[5.67,6.3,8.58,4.35,2.67],"433":[9.17],"435":[8.26,6.2],"436":[3.97],"437":[2.49,3.11,9.06,1.29],"438":[1.6800000000000002,1.72],"439":[9.46],"440":[4.13],"441":[1.85,6.93],"442":[3.23],"443":[4.62,3.38],"444":[2.8899999999999997,9.86],"446":[8.530000000000001,8.71],"447":[7.93,5.1,4.58],"448":[3.69],"449":[3.09],"450":[7.92],"451":[5.64],"452":[6.42,8.620000000000001],"453":[1.53,8.379999999999999],"454":[8.57,9.69,7.5],"455":[7.29,5.38],"456":[3.97],"457":[4.59,6.1],"458":[9.86,3.45,4.21,1.54,3.32,3.25],"459":[4.779999999999999,3.19],"460":[8.07,3.57,3.19,8.469999999999999,3.91],"461":[9.43,7.97,2,3.02],"462":[8.42],"463":[4.91],"464":[2.75,9.1,9.66,3.25,8.68],"465":[6.96,7.86,6.75],"467":[9.95],"469":[9.96],"470":[9.33,6.21,9.19,9.86,7.51,2.9],"471":[3.86],"472":[2.9299999999999997,8.57,6.13],"473":[1.67,4.220000000000001,9.15,1.7],"474":[2.26,9.31],"476":[1.2],"478":[7.76],"479":[9.23],"480":[7.94],"482":[1.7],"483":[7.8,6.73],"484":[2.4299999999999997,2.4,5.75],"485":[7.6],"486":[9.96],"487":[7.52],"488":[6.52],"490":[8.86],"491":[1.54,4.8],"492":[8.16],"493":[3.74,9.93,1.5899999999999999,8.440000000000001],"495":[5,5.53,8.27],"496":[6.47,3.78,3.54],"497":[3.89,9.04],"498":[2.2199999999999998,5.07],"499":[5.38]},"drop":{"0":[0,0,1],"1":[0,0,1],"2":[1,1,1],"3":[0,2,2],"4":[0,0,1],"5":[1,1,2],"6":[4,0,0],"7":[2,2,1],"8":[1,0,2],"9":[0,1,0],"10":[1,0,1],"11":[1,5,3],"12":[1,3,0],"13":[4,2,1],"14":[3,0,0],"15":[0,1,1],"16":[0,2,1],"17":[3,1,1],"18":[1,0,1],"19":[1,0,1],"20":[4,1,1],"21":[0,0,2],"22":[0,1,0],"23":[0,1,3],"24":[0,1,0],"25":[2,2,1],"26":[1,1,2],"27":[0,1,4],"28":[0,1,0],"29":[0,3,1],"31":[1,1,1],"32":[3,1,2],"33":[2,1,0],"34":[1,0,3],"35":[1,0,2],"36":[0,1,3],"37":[2,0,1],"39":[0,1,0],"40":[1,0,0],"41":[0,2,1],"42":[2,1,3],"43":[1,3,1],"44":[1,1,0],"45":[3,2,1],"46":[1,1,0],"47":[0,0,4],"48":[2,2,2],"49":[0,0,2],"50":[1,1,0],"51":[1,0,0],"52":[0,1,0],"53":[1,2,3],"54":[0,1,0],"55":[0,2,4],"56":[0,1,1],"57":[1,1,1],"58":[1,0,0],"59":[0,1,3],"60":[0,1,0],"61":[1,1,0],"62":[2,2,1],"63":[0,1,1],"64":[3,2,1],"65":[0,2,1],"66":[1,2,2],"67":[0,0,2],"68":[1,2,0],"69":[2,0,3],"70":[2,2,1],"71":[1,2,2],"72":[1,0,2],"73":[0,2,2],"74":[1,2,2],"75":[0,0,3],"76":[2,1,0],"77":[1,1,1],"78":[2,2,1],"79":[3,1,0],"80":[0,2,0],"81":[0,0,1],"82":[2,0,1],"83":[1,0,2],"84":[1,2,1],"85":[0,1,2],"86":[1,1,0],"87":[0,3,1],"89":[0,1,2],"90":[3,1,0],"91":[0,1,0],"92":[0,2,1],"93":[2,1,2],"94":[1,1,0],"95":[0,0,1],"96":[0,0,1],"97":[1,1,0],"98":[0,1,2],"99":[1,0,2],"100":[3,3,2],"101":[1,2,1],"102":[4,1,0],"103":[1,3,0],"104":[1,1,2],"105":[1,1,0],"106":[1,0,1],"107":[0,1,0],"108":[3,0,1],"109":[0,1,3],"110":[0,0,1],"111":[1,1,3],"112":[3,2,2],"113":[1,0,0],"114":[0,2,1],"115":[0,0,2],"116":[0,1,0],"117":[0,0,2],"118":[0,1,3],"119":[0,1,1],"120":[1,0,1],"121":[0,2,2],"122":[0,1,0],"124":[1,1,1],"125":[3,1,3],"126":[0,0,1],"127":[1,2,1],"128":[2,0,1],"129":[0,0,1],"130":[1,1,0],"131":[0,0,2],"134":[3,1,1],"135":[1,3,0],"136":[1,3,1],"137":[1,0,4],"138":[3,1,0],"139":[3,1,0],"140":[1,0,1],"141":[1,0,0],"142":[3,0,1],"143":[0,3,1],"144":[1,0,3],"145":[2,0,0],"146":[0,1,4],"147":[1,2,1],"148":[2,1,1],"149":[0,0,1],"150":[3,2,0],"151":[0,0,1],"152":[1,0,2],"153":[2,2,2],"154":[0,1,1],"155":[0,1,1],"156":[2,1,1],"157":[0,2,2],"158":[1,0,1],"159":[1,0,0],"160":[1,1,2],"161":[2,1,2],"162":[2,0,0],"163":[0,2,1],"164":[0,3,4],"165":[0,1,0],"166":[0,1,1],"167":[2,1,0],"168":[0,2,0],"169":[0,2,0],"170":[1,1,1],"171":[2,1,2],"172":[0,1,0],"173":[1,0,0],"174":[0,1,1],"175":[0,0,2],"176":[2,2,1],"177":[1,1,0],"178":[1,1,2],"179":[2,2,0],"180":[0,2,0],"181":[2,0,1],"182":[1,0,1],"183":[1,0,0],"184":[2,1,3],"185":[1,2,0],"186":[0,1,0],"187":[2,1,1],"188":[1,1,2],"189":[1,3,3],"190":[1,2,1],"191":[0,0,1],"192":[0,2,1],"193":[2,0,1],"194":[1,2,1],"195":[0,1,2],"196":[1,0,3],"197":[0,1,3],"198":[0,1,0],"199":[0,2,1],"200":[0,1,1],"201":[0,0,1],"202":[1,0,1],"203":[3,1,2],"204":[2,1,2],"205":[0,2,1],"206":[0,0,1],"207":[0,1,3],"208":[2,0,0],"209":[2,1,2],"210":[0,2,3],"211":[0,0,2],"212":[0,3,0],"213":[0,1,1],"214":[2,3,0],"215":[1,0,1],"216":[0,0,2],"217":[1,3,0],"218":[1,2,0],"219":[0,1,0],"220":[0,0,2],"222":[2,1,1],"223":[1,2,2],"224":[1,0,0],"225":[2,4,0],"226":[0,1,1],"227":[1,1,1],"228":[1,1,1],"229":[3,2,0],"230":[1,1,2],"231":[2,1,1],"232":[1,0,0],"235":[3,1,2],"236":[2,1,3],"237":[2,1,1],"238":[1,2,1],"239":[1,0,1],"240":[2,1,2],"241":[2,1,1],"242":[1,3,2],"243":[2,1,2],"244":[0,3,0],"245":[0,0,1],"246":[1,0,2],"247":[2,0,0],"248":[2,0,0],"249":[0,0,1],"250":[1,2,0],"251":[0,0,1],"252":[0,1,2],"253":[2,0,1],"254":[4,0,1],"255":[1,1,1],"256":[0,3,0],"257":[2,0,0],"258":[1,0,0],"259":[1,1,2],"260":[2,1,0],"261":[4,0,0],"262":[2,1,1],"263":[0,1,1],"264":[0,1,3],"265":[0,1,1],"266":[1,0,2],"267":[1,2,1],"268":[1,1,3],"269":[1,1,0],"270":[3,1,3],"271":[3,0,0],"272":[1,2,0],"273":[0,1,1],"274":[0,0,1],"275":[1,0,1],"276":[4,1,1],"277":[2,0,0],"278":[0,3,1],"279":[1,2,0],"280":[1,2,2],"281":[0,2,2],"282":[0,1,2],"283":[1,1,1],"284":[0,2,0],"285":[1,0,2],"286":[2,0,1],"287":[1,1,4],"289":[1,0,4],"290":[0,0,1],"291":[0,1,0],"292":[0,2,1],"293":[1,2,0],"294":[2,2,0],"295":[1,4,0],"296":[2,2,1],"297":[1,1,2],"298":[0,0,2],"299":[3,0,2],"300":[0,1,0],"301":[3,0,1],"302":[0,0,1],"303":[1,1,0],"304":[0,0,2],"306":[1,1,2],"307":[2,0,0],"308":[1,1,4],"309":[1,1,2],"310":[2,0,1],"311":[0,2,1],"312":[1,0,0],"313":[1,0,1],"314":[0,3,1],"315":[2,0,2],"316":[1,1,2],"317":[1,2,2],"318":[0,1,0],"319":[0,1,0],"320":[0,1,2],"321":[0,0,2],"322":[2,1,1],"323":[2,0,2],"324":[1,1,0],"325":[0,2,0],"326":[0,1,0],"327":[0,2,1],"328":[1,1,1],"329":[0,1,0],"330":[2,2,2],"331":[1,0,0],"332":[2,1,0],"333":[2,0,1],"334":[1,3,0],"335":[0,1,1],"336":[2,0,3],"337":[3,1,0],"338":[2,0,2],"339":[0,2,1],"340":[2,1,0],"341":[1,1,3],"342":[2,0,0],"343":[1,0,1],"344":[0,1,2],"345":[1,1,1],"346":[2,0,0],"347":[0,1,2],"348":[1,2,0],"349":[0,0,1],"350":[0,1,1],"352":[2,2,1],"353":[2,4,0],"354":[0,3,3],"355":[1,0,2],"356":[0,0,2],"357":[1,1,0],"358":[0,1,1],"359":[3,1,0],"360":[0,1,2],"361":[0,3,0],"362":[0,4,1],"363":[0,1,2],"364":[1,1,0],"365":[0,1,0],"366":[0,1,0],"367":[1,0,0],"368":[0,0,1],"369":[0,1,1],"370":[1,4,0],"371":[1,0,1],"372":[0,2,3],"373":[0,0,3],"374":[2,2,2],"375":[2,1,2],"376":[0,3,2],"377":[0,2,2],"378":[2,0,1],"379":[2,1,1],"380":[2,2,0],"381":[2,0,2],"382":[1,0,2],"383":[1,0,1],"384":[1,2,1],"385":[3,0,1],"387":[1,2,0],"388":[1,0,3],"389":[0,0,1],"390":[1,0,1],"391":[1,0,1],"393":[0,0,1],"395":[0,1,5],"396":[3,1,1],"397":[0,1,0],"398":[1,4,2],"399":[0,0,1],"400":[1,2,1],"401":[0,2,0],"402":[1,0,0],"403":[0,1,2],"404":[0,2,1],"405":[2,1,0],"406":[0,0,1],"407":[0,3,0],"408":[1,2,0],"409":[1,0,3],"410":[1,4,0],"411":[0,0,1],"412":[0,2,1],"413":[1,2,1],"414":[0,2,1],"415":[0,1,2],"416":[0,1,2],"417":[2,1,4],"418":[2,3,0],"419":[2,0,2],"420":[1,3,0],"421":[1,1,1],"422":[3,1,1],"423":[0,0,2],"424":[3,1,2],"425":[1,1,5],"427":[1,1,2],"428":[2,0,1],"429":[1,0,1],"430":[1,0,1],"431":[2,0,1],"432":[0,2,0],"433":[1,0,3],"434":[0,2,0],"435":[3,1,0],"436":[0,2,0],"437":[2,0,0],"438":[1,1,1],"439":[1,0,2],"440":[2,0,0],"441":[1,1,2],"442":[2,0,2],"443":[1,1,0],"444":[0,2,1],"445":[0,1,0],"447":[0,1,1],"448":[4,0,1],"449":[0,1,3],"450":[1,1,0],"451":[1,2,1],"452":[0,1,1],"454":[1,0,0],"455":[2,2,2],"456":[4,2,1],"457":[2,3,1],"458":[0,1,1],"459":[0,1,2],"460":[0,2,2],"461":[1,2,0],"462":[0,2,1],"463":[0,1,1],"464":[0,1,0],"465":[3,1,2],"466":[1,0,2],"467":[0,0,2],"468":[1,1,2],"469":[4,1,1],"470":[1,1,2],"471":[2,1,0],"472":[0,1,1],"473":[2,3,1],"474":[1,1,2],"475":[1,0,0],"476":[0,1,1],"477":[3,0,3],"478":[0,1,1],"479":[0,1,0],"480":[1,0,0],"481":[1,0,1],"482":[0,0,1],"483":[2,3,2],"484":[1,1,1],"485":[0,1,1],"486":[4,0,0],"487":[1,1,4],"488":[1,1,2],"489":[1,1,1],"490":[0,1,1],"491":[0,2,0],"492":[1,2,0],"493":[0,1,0],"494":[4,1,0],"495":[2,0,1],"496":[1,0,2],"497":[3,4,1],"498":[0,1,4],"499":[1,2,0]}}

and here's generation 9 
{"effort":{"0":[1.18,9.13],"7":[9.93],"8":[5.41],"10":[1.18],"15":[2.41],"17":[4.87],"24":[6.63,6.73,6.73,6.73,6.73,6.73,6.73,6.73,6.73,6.73,6.73,6.73,6.73],"28":[7.81],"30":[8.629999999999999,1.26],"34":[4.04],"39":[5.67],"43":[3.63],"45":[5.32],"48":[9.61],"49":[9.44],"51":[8.440000000000001],"52":[1.69],"55":[8.43],"66":[7.57],"69":[6.1,6.17],"74":[3.02],"78":[6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1,6.1],"80":[2.55],"82":[9.32,3.22],"84":[8.91,8.469999999999999],"86":[8.32],"88":[6.1],"96":[6.7],"98":[7.3,6.83],"106":[2.1399999999999997],"107":[8.5],"109":[9.13,9.13,9.13,9.13,9.13],"115":[4.4],"116":[3.55],"117":[2.3899999999999997],"121":[3.49],"122":[2.8600000000000003,9.53,4.21],"123":[9.45],"126":[6.51],"127":[5.95,5.95,5.95,5.95,5.95,5.95],"130":[3.26],"133":[8.780000000000001],"134":[6.14,3.23,6.14,6.14,6.14,6.14,6.14,6.14,6.14],"136":[2.67],"143":[9.97],"153":[7.72,9.92],"154":[3.8],"160":[4.77],"161":[2.94],"162":[6.75],"165":[6.24],"169":[1.87,7.34],"172":[9.39],"176":[6.11],"183":[3.14,6.33,3.14,3.14,3.14,3.14,6.33,6.33,6.33,6.33,6.34],"187":[5.63,5.63,5.63,5.63,5.63,5.63],"191":[9.27],"195":[6.1],"197":[3.27],"198":[5.42],"200":[7.13,7.13,7.13,7.13,7.13,7.13,7.13],"203":[1.9],"208":[6.43],"209":[2.25],"217":[2.26],"220":[4.8100000000000005],"221":[6.09],"223":[5.12],"227":[9.9],"240":[7.46],"253":[3.34],"264":[7.39],"269":[7.64],"272":[8.66,8.66,8.66,8.66,8.66,8.66],"275":[8.58,8.58,8.64,8.58,8.58,8.58,8.58],"284":[3.48],"285":[6.71,6.71,6.71],"290":[6.24],"292":[8.879999999999999,3.6],"299":[8.25]},"drop":{"3":[0,1,0],"5":[0,0,1],"6":[0,1,0],"7":[0,0,1],"8":[0,1,0],"10":[0,0,1],"11":[0,14,0],"13":[20,3,0],"15":[0,0,1],"16":[1,0,0],"22":[2,0,0],"23":[1,0,0],"27":[0,1,0],"28":[0,0,1],"29":[0,8,0],"32":[0,0,1],"34":[1,0,0],"35":[0,2,1],"36":[0,1,1],"38":[0,0,16],"39":[1,0,0],"40":[1,0,0],"42":[0,6,0],"46":[0,0,1],"51":[0,1,0],"53":[1,0,0],"54":[0,1,0],"56":[0,11,0],"58":[1,0,0],"59":[0,1,0],"60":[1,0,1],"62":[1,0,0],"63":[1,0,0],"65":[0,0,1],"67":[0,6,0],"68":[6,0,7],"71":[0,0,5],"73":[1,0,0],"77":[0,9,0],"79":[0,0,1],"81":[1,0,19],"83":[0,0,1],"85":[1,6,0],"91":[0,1,0],"95":[1,0,0],"96":[0,0,1],"97":[1,0,0],"100":[24,0,0],"102":[14,0,1],"103":[1,15,0],"105":[1,0,1],"107":[0,7,0],"108":[1,4,0],"109":[1,0,1],"110":[1,7,0],"111":[4,0,1],"115":[2,0,0],"116":[0,0,1],"117":[1,0,0],"119":[1,0,0],"120":[0,0,5],"121":[0,1,0],"123":[1,0,0],"125":[0,1,0],"127":[0,1,0],"128":[0,1,0],"130":[0,0,1],"131":[39,0,1],"134":[0,5,6],"137":[1,0,18],"138":[0,0,1],"141":[0,0,2],"142":[1,0,1],"143":[1,1,1],"144":[2,0,0],"145":[0,1,0],"147":[1,0,1],"149":[2,0,0],"150":[0,1,0],"154":[0,0,1],"155":[0,0,1],"164":[0,1,1],"167":[0,0,2],"169":[0,1,0],"170":[4,5,0],"171":[1,0,0],"176":[1,0,0],"179":[1,0,1],"181":[6,0,11],"183":[0,0,1],"184":[0,2,0],"186":[0,0,6],"187":[0,2,0],"194":[1,0,0],"197":[0,2,0],"199":[0,1,0],"203":[0,0,2],"206":[0,1,1],"207":[0,1,0],"209":[0,1,0],"210":[0,0,57],"214":[0,1,0],"216":[1,0,0],"217":[1,0,0],"219":[1,0,1],"224":[1,0,0],"226":[0,0,1],"229":[0,0,5],"234":[0,0,1],"240":[13,9,0],"245":[1,1,0],"249":[0,3,0],"253":[1,0,0],"257":[18,0,0],"259":[1,1,1],"262":[1,0,13],"264":[0,0,1],"266":[0,0,1],"268":[0,0,1],"270":[0,0,2],"272":[0,2,0],"274":[1,0,0],"279":[0,0,1],"280":[0,1,0],"281":[1,0,0],"282":[1,0,1],"288":[1,0,0],"290":[0,0,1],"292":[0,0,4]}}

-- massive loss of diversity there. lots of data too even with a population of just 100. Does it still run with a pop of 1000?

-- let's save it first before looking at the scalability issue.
-- do I stringify here? maybe all the results object is stringified later?

  ga_results.generations.push(generation_results); -- here is where they are pushed into a results object.
  
-- right, so it seems to be saving now... too much data but jsut keep going, need to get it back out in the results page via a chart dropdown or button in the table? I think build a custom raw data graph that outputs all of the logged gens

-- ok, so in PyCharm now... need to draw a scatter plot and set the bubble size using the frequency of a value?

-- e.g. df.plot.scatter(x="a", y="b", s=df["c"] * 200);

-- ok, have some charts now, but need 'real' data for 50 gens of 0.6 noise and 0 noise?

-- push the data into text files, wayyyy too big

-- wednesday 13th... so much to do. Read the existing paper sections again first.

-- run the same experiment as yesterday except with all weak + noise. Do we have the same issues with instruction reduction and convergence towards the end?

-- different bahaviour with all weak - there are popular instructions near the beginning. Finding that an effort instruction can be essentially ignored if it closely follows a DROP, as the rider does not have enough time to actually accelerate to the target speed. In an all-weak environment the drop instruction has no real cost so protects from noise? doesn't even matter what amount it drops by!

-- generate the gen 49 effort and drop instruction distribution graphs for ALL WEAK noise 0/0.3/0.5/1

-- also run this for 1 STRONG and 2 STRONG

--

allWeak0NoiseGen49Effort.json
allWeak0NoiseGen49Drop.json
allWeak0p3NoiseGen49Effort.json
allWeak0p3NoiseGen49Drop.json
allWeak0p5NoiseGen49Effort.json
allWeak0p5NoiseGen49Drop.json
allWeak1NoiseGen49Effort.json
allWeak1NoiseGen49Drop.json

oneStrong0NoiseGen49Effort.json
oneStrong0NoiseGen49Drop.json
oneStrong0p3NoiseGen49Effort.json
oneStrong0p3NoiseGen49Drop.json
oneStrong0p5NoiseGen49Effort.json
oneStrong0p5NoiseGen49Drop.json
oneStrong1NoiseGen49Effort.json
oneStrong1NoiseGen49Drop.json

twoStrong0NoiseGen49Effort.json
twoStrong0NoiseGen49Drop.json

twoStrong0p3NoiseGen49Effort.json
twoStrong0p3NoiseGen49Drop.json

twoStrong0p5NoiseGen49Effort.json
twoStrong0p5NoiseGen49Drop.json

twoStrong1NoiseGen49Effort.json
twoStrong1NoiseGen49Drop.json

-- got colour in scatter to turn red. redo all the recent graphs?
-- save them all to
C:\Users\donak\Documents\RESEARCH\images\noiseTestsMarchApril22\allWeak
C:\Users\donak\Documents\RESEARCH\images\noiseTestsMarchApril22\OneStrong
C:\Users\donak\Documents\RESEARCH\images\noiseTestsMarchApril22\TwoStrong


-- Tuesday 19th. Need to do a full hour with maximum focus. Open the paper and read the last sentence.

-- generate the average number of instructions graphs for one-strong/two
-- also want to generate the original landscape graph for each of the 4 scenarios as well. 

-- did the avg instructs ones, confused with the landscape one, code is messy and I don't understand the plotting again

-- add titles, and labels to the green dots one.

-- interesting way to avoid noise- add an effort instruction immediately followed by a drop, so the full power is not reached, and if you set the effort to any higher value it doesn't wreak things 

[[3,"effort=8.15"],[8,"drop=2"],[57,"drop=2"],[61,"drop=1"],[83,"drop=1"],[121,"drop=1"],[155,"drop=1"],[164,"drop=1"],[200,"drop=2"],[203,"drop=1"],[234,"drop=3"],[252,"effort=8.809999999999999"],[253,"effort=6.65"],[261,"drop=1"]]

result id 6256e9b99a7c522688ccf351

-- wednesday 

-- redo the ThreeStrong landscape 3d graph with a header...

-- done, had to do it twice.

-- put in a section that contains graphs for all weak/1 strong/2 strong, side by side.

-- two of these seem identical :-( soooo which one is correct?

-- turds day 21 april 
-- fix the identical graph issue

don't seem to have the ALL Weak landscape?

-- fry 22 april 
-- added a paragraph and 3 more graphics, there's a 3*3 grid one now. 

-- need to add code to average some races instead of just takign one... will have to add another global setting.

-- added setting "number_of_races_to_average":3

-- wait... if I use an average, then I lose the recording of noise and suchlike? Or I only record the LAST one?

-- at least give it a try,save the last one etc.

-- hmmm, seeeems to kinda work.

-- ran it with pop 3000, 50 gens, 3 strong... ends up with just 3 instructos. so it will just pressure out the instructions? 

-- but it i run it for the whole range of noise values will it provide a different landscape?

-- wednesday 27th 

-- get graphs for the average of 2 and 3 race data, need to be able to analyse those and compare them... remember that we can't exactly replicate things anymore as only the noise for the last run will be saved, so we don't know the full info behind the fitness.

-- might as well make the instruction distribution graphs too 

fitness_avg_3_3Strong0NoiseGen49Effort.json
fitness_avg_3_3Strong0NoiseGen49Drop.json

fitness_avg_3_3Strong0p3NoiseGen49Effort.json
fitness_avg_3_3Strong0p3NoiseGen49Drop.json

fitness_avg_3_3Strong0p5NoiseGen49Effort.json
fitness_avg_3_3Strong0p5NoiseGen49Drop.json

fitness_avg_3_3Strong1NoiseGen49Effort.json
fitness_avg_3_3Strong1NoiseGen49Drop.json

fitness_avg_3_3Strong1NoiseGen49Drop seems weird, looks like drops all over the placements
BUT 
more like there is no clustering of drops at the end. so all there is are the random mutos... ran it a 2nd time and there IS a popular drop or two. odd.

-- run this for an average of 6 races?

-- friday 29

need a title for the grc thiingy

In Search of Fast Times: Evolving Strategies for a Team Pursuit Bike Race.

-- Tuesday May 03. Hendon. GRC prep?

-- look at couple of algorithms, for overleaf doc?

-- review paper 2... need to add new forms of noise?n

-- make a dummy version of the GRC presentation?



-- get some kind of different template (form over function)

-- got a template, created a front slide, kinda stuck now on the introduction. Need about twenty slides. 

-- let's at least start the choking under pressure idea... add a property, code something?

@article{Williams.2020,
 author  = { Williams, M., A., and Wigmore, T.},
 date    = {2020-11-05},
 title   = {Under pressure: why athletes choke},
 journal = {The Guardian},
 url     = {https://www.theguardian.com/sport/2020/nov/05/under-pressure-why-athletes-choke},
 urldate = {2022-05-03}
}

The high pressure of some environments ane scenarios may cause unexpected drops in performance, i.e. that factors such as anxiety trigger a collapse of sorts in ability levels. This is often called `choking': an example described by CITE

stereotype threat (mayo football in finals)
clutch state/flow state

-- described chokign under presure, but what do I actually build?

Give each race a pressure rating? Or have a pressure point inside the races? Needs to be stochastic but settable? Need to record enough info to replicate it? Or is it just added as a noise with a new type? How does it work with the other forms of noise? can they all happen at once? 

need to set the probability of choking somehow based on how the race is going and rider/race/general properties 
- more pressure on rider at front
- more pressure when instructions are given
- arbitrarily give each race a pressure level?

- each rider works out at each time step if they are going to choke or not. 
- choking causes a rider to get worse, i.e. their threshold and max decrease?
- once a rider starts to choke they cannot just stop at the next time-step, sooo, it's like a switch. The rider's ability gets comprimised.

-- add a global setting
"enable_pressure_noise":0 (0 or 1)


-- add a rider setting
"pressure_probability_per_timestep": 0 (0 to 1, 2 decimals? e.g. 0.05 or 0.6). needs to verrrry low!

-- how to we make SOME races more pressurised? set a property at the beginning of each?
-- maybe build a simpler version first, where it's all down to the rider probability. but once they flip there has to be an amount that they worsen by. a

"pressure_failure_amount": 0.1 (10% reduction in both threshold and max)

-- so we still also need to factor in the race pressure... but it needs to be based on how well they are doing, i.e. they know they are in a good position and know that they MIGHT screw it up- there has to be something to screw up... so maybe if it looks like they can set a new PB then the risk goes up?

defien it as a range, from 1 to 3 or something... be careful never to end up with crazy high P 


-- default pressure_of_fast_race_factor = 1.
"min_pressure_of_fast_race_factor" = 1
"max_pressure_of_fast_race_factor" = 3

how do we set a ceiling on that? Is it 3 if they are on course for a faster time no matter how much faster?

if generation > 1
get best race speed from last generation 
get average speed

prob of choking should increase as the end approaches and they are on target for a new PB?


if (enable_choking_noise)
	if !(rider.under_pressure)
		pressure_of_fast_race_factor = =((POW(avg_speed,2)/POW(best_avg_speed_last_gen,2)) + (POW(current_timestep,2)/POW(finish_time_last_race,2)))/2
		pressure_prob = pressure_probability_per_timestep*(pressure_of_fast_race_factor)
		if rand() < pressure_prob
			rider.under_pressure = 1
			
			
-- when applying power			
if(enable_choking_noise)
	if(rider.under_pressure)
		max_power = max_power - (max_power*pressure_failure_amount)
		threshold_power = threshold_power - (threshold_power*pressure_failure_amount)
		
-- furst task... if enable_choking_noise is on, log the prev. gen best time (gen > 1)
			
			
			race_rider.distance_covered
-- done, now need to work out the actual pressure stuff 

pressure_prob 2.991502802804618 pressure_of_fast_race_factor 2991.502802804618 pressure_probability_per_timestep 0.001 max_pressure_of_fast_race_factor 3 min_pressure_of_fast_race_factor 1 speed_of_best_race_found_thus_far 0.06874341666666668 speed_this_race_thus_far 5.317295765991861

bad calc 1 

 pressure_prob 0.000230380844068964 pressure_of_fast_race_factor0.230380844068964 pressure_probability_per_timestep 0.001 max_pressure_of_fast_race_factor 3 min_pressure_of_fast_race_factor 1 speed_of_best_race_found_thus_far 13.852541999175775 speed_this_race_thus_far 0.07169624588285681
 
 ((((((((((((  pressure calculation p232 0.0004578180873613391  ))))))))))))
race_function_no_vis.js:1973  pressure_prob 0.0005428365018631494 pressure_of_fast_race_factor0.5428365018631495 pressure_probability_per_timestep 0.001 max_pressure_of_fast_race_factor 3 min_pressure_of_fast_race_factor 1 speed_of_best_race_found_thus_far 14.572603747830808 speed_this_race_thus_far 0.08192631227387154

-- next need to actually reduce the power... needs a lot of checking

-- wednesday may 4. changing the power but lotsa errors methinks, e.g. the -4000 here:
pressure_prob 8.320500000000164 pressure_of_fast_race_factor8320.500000000164 pressure_probability_per_timestep 0.001 max_pressure_of_fast_race_factor 3 min_pressure_of_fast_race_factor 1 speed_of_best_race_found_thus_far -4000 speed_this_race_thus_far 0.07240362026386313

pressure_of_fast_race_factor_level 0.14699117510392748 pressure_prob 0.0012939823502078548 pressure_of_fast_race_factor1.293982350207855 pressure_probability_per_timestep 0.001 max_pressure_of_fast_race_factor 3 min_pressure_of_fast_race_factor 1 speed_of_best_race_found_thus_far 12.908597448615716 speed_this_race_thus_far 0.08533316814338968

pressure_of_fast_race_factor_level 0.08901444459093538 pressure_prob 0.0011780288891818707 pressure_of_fast_race_factor1.1780288891818707 pressure_probability_per_timestep 0.001 max_pressure_of_fast_race_factor 3 min_pressure_of_fast_race_factor 1 speed_of_best_race_found_thus_far 13.081371581810352 speed_this_race_thus_far 0.0934382505019717
race_function_no_vis.js:1983 OLD threshold_power 500 max_power 1200
race_function_no_vis.js:1987 UPDATED threshold_power 450 max_power 1080

should the actual choking thing be a fixed amount?

why is the current average speed so slow?


speed_this_race_thus_far 0.09658470738963205race_r.race_clock 194 race_rider.distance_covered2008.5995520738622 
nop nop nope, calculation is befuddled again, speed is distance/time ya eejit

((((((((((((  pressure calculation p232 0.00038573245841044823  ))))))))))))
race_function_no_vis.js:1980  pressure_of_fast_race_factor_level 0 pressure_prob 0.001 pressure_of_fast_race_factor1 pressure_probability_per_timestep 0.001 max_pressure_of_fast_race_factor 3 min_pressure_of_fast_race_factor 1 speed_of_best_race_found_thus_far -4000 speed_this_race_thus_far 12.126793319158468race_r.race_clock 173 race_rider.distance_covered2097.935244214415
race_function_no_vis.js:1983 OLD threshold_power 500 max_power 1200
race_function_no_vis.js:1987 UPDATED threshold_power 450 max_power 1080

-- can I create a graph showing how the fatigue and recovery mechanisms actually work?

e.g. run a race where the rider is told to go at maximum all the time and never changes position. what does the power graph look like?

[
[5,"effort=9"],
[10,"effort=9"],
[15,"effort=9"],
[20,"effort=9"],
[25,"effort=9"],
[30,"effort=9"],
[35,"effort=9"],
[40,"effort=9"],
[45,"effort=9"],
[50,"effort=9"],
[55,"effort=9"],
[60,"effort=9"],
[65,"effort=9"],
[70,"effort=9"],
[75,"effort=9"],
[80,"effort=9"],
[85,"effort=9"],
[90,"effort=9"],
[95,"effort=9"],
[100,"effort=9"],
[105,"effort=9"],
[110,"effort=9"],
[115,"effort=9"],
[120,"effort=9"],
[125,"effort=9"],
[130,"effort=9"],
[135,"effort=9"],
[140,"effort=9"],
[145,"effort=9"],
[150,"effort=9"],
[155,"effort=9"],
[160,"effort=9"],
[165,"effort=9"],
[170,"effort=9"],
[175,"effort=9"],
[180,"effort=9"],
[185,"effort=9"],
[190,"effort=9"],
[195,"effort=9"],
[200,"effort=9"],
[205,"effort=9"],
[210,"effort=9"],
[215,"effort=9"],
[220,"effort=9"],
[225,"effort=9"],
[230,"effort=9"],
[235,"effort=9"],
[240,"effort=9"],
[245,"effort=9"],
[250,"effort=9"],
[255,"effort=9"],
[260,"effort=9"],
[265,"effort=9"],
[270,"effort=9"],
[275,"effort=9"],
[280,"effort=9"],
[285,"effort=9"],
[290,"effort=9"],
[295,"effort=9"],
[300,"effort=9"],
[305,"effort=9"],
[310,"effort=9"],
[315,"effort=9"],
[320,"effort=9"],
[325,"effort=9"],
[330,"effort=9"],
[335,"effort=9"],
[340,"effort=9"],
[345,"effort=9"],
[350,"effort=9"],
[355,"effort=9"],
[360,"effort=9"],
[365,"effort=9"],
[370,"effort=9"],
[375,"effort=9"],
[380,"effort=9"],
[385,"effort=9"]
]

hmmm, need to get the power graph to output the actual data so I can muck with it in Python? In the game screen.
$('#data_display').val();


hmmmmmmm

two nasty problems

1: in most of the 'new' experiments, "power_adjustment_step_size_down":-100, but in the old, "power_adjustment_step_size_down":-40,

And

if I make recovery very slow, something goes weird. the first recovery takes a while, then the maxmimum power reachable is much much lower. why? Is the accumulated fatigue having that much of an effect? 

with a high fatigue rate a rider cannot reach its max power


I should find the work on Rl where a speedboat in a game goes round in circles to exploit a gap in the game and its expected play. Hmmm. https://openai.com/blog/faulty-reward-functions/ Coast Runners. OPen AI. 

October 10. don't mention the gaps.

Need to finish paper 2 and build a race-finish simulator.... just follow your interest and at least try. will need to remodel how the instructions work and hhow the strategy is expressed. Surely it is possible? 

> flat road, no bends. 

> two groups, chasers and break. break knows how far behind the bunch is. if they slack off they will be caught. but they also have to compete with each other, saving energy for the sprint or attacking from far out. So I have to evolve the strategy of 1 rider while fixing the others? i.e. make some rider types, and gradually build them up. 

BUT the strategy can't be as simple as just DO X at time Y. This is actualluy the key part- the strategy has to be social, dependent on the others in the group AND the distance/expected time remaining. Ok though to evolve e.g. the best strategy for a sprinter, or for a TT specialist.

-- October 24. Still not doing stuff.

-- Dec 5. viddy well. Be that person. You know, that person.

-- Where was i with the choking-under-pressure?

-- need to build a performance-pressure mechanism. then add a race pressure and a rider property and a switch to turn this whole thing on
-- 1: add a new general setting "choke_under_pressure_switch": 1,
-- get it to notice this and log a message (no real work yet)

-- Dec 6. That person. A technical manual. He write poetry in his code comments. She is sleeping. 
-- general setting is seen. Now need to need to develop a formula (in Excel?) for a function that will return the amount of failure.
-- this kind of failure compounds, right? Or, it's like a switch, hard to turn off once it starts? Not a step by step thing? Maybe that's a problem with all of the noise I've implemented?

Dec 12th

-- maybe if the rider fails you just lower its ability by a percentage? like, both max and threshold?

-- how to work out the pressure of a race? average speed, closeness to end, 
-- or estimate the expected finish time and that + the % of time left to get there, e.g. 200 seconds, 0.5 remaining. Current PB: 201s

-- so I need current_timestep, current_best_time, current_distance. needs to be tolerant of crudity. 

-- assumption: if a rider chokes it will remain in that state until the end of the event/race, then reset for the next one.

-- should be possbile to configure it so that a rider ALWAYS chokes, i.e. by maxxing out the values, or NEVER chokes, by minning them 

-- if there is NO current_best time, choking should be impossibile; pressure shouldn't exist? or should it just be there but minimal?

-- first attempt: rider_cup_prob * race_cup_pressure * (% of race done) * (speed > best speed)

-- will give 0 for all cases where speed < best speed (dumb). but this has to be dumb. a basic constraint. should i have the rider's intensity baked in? 

-- need to make it polynomial? hmmmm. i hasn't a clue. but need to get this paper done somehow...

-- maybe replicate the approach i used with performance noise... actually wasn't that kinda the same? there, we modelled the idea of intensity, here it is pressure... but intensity is one dimension of pressure?? 

-- mebbie at least build the function and the switch idea, then can tweak that... can set an upper and lower boundary and derive an expression that increases from one to the other

(cup_race_pressure^cup_race_pressure_exp) / (max_cup_race_pressure^cup_race_pressure_exp)

-- maybe to think it through make a function that takes a set of properties as a list 

-- built a function called calculate_linear_space_value(). Need to test it then..... use it to work out a prob, then actually build the main failure behaviour. oh. dear. need to test the crap out of it.

-- dec 13

-- need to test the function i created yesterday.
-- also n eed to commit to git
-- really need to relearn git. so much going on. 

-- create some test examples
-- let value_list = [1,5,1,10,1,5,1,10,1,10,1,10]
-- let probability_variables = [0.6]

-- getting the % improvement over the global best time as a property here
-- can be negative, and in most cases WILL be negative; what will that do?
-- let value_list = [1,-0.01,2,1]

-- Jan 16th
-- finish choking under pressure noise!
-- calculate_linear_space_value(value_list, probability_variables)